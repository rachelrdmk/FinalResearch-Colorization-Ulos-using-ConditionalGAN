{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMAGE COLORIZATION USING cGAN\n",
    "\n",
    "In this notebook we are going to use HSV colorspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Contents**\n",
    "\n",
    "1. [Import Packages](#packages)\n",
    "2. [Utils](#utils)\n",
    "3. [Data preparation](#dataset)\n",
    "4. [Image Colorization and Color Science Computation](#4-image-colorization-and-color-science-computations)\n",
    "5. [Generator architecture](#generator)\n",
    "6. [Discriminator architecture](#discriminator)\n",
    "7. [Trainer](#training)\n",
    "8. [Validation](#validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Packages <a class=\"anchor\" id=\"packages\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Operating system and file operations\n",
    "import os\n",
    "\n",
    "# Numerical operations and array processing\n",
    "import numpy as np\n",
    "\n",
    "# Data visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Random number generation\n",
    "import random\n",
    "\n",
    "# Image processing and computer vision\n",
    "import cv2\n",
    "\n",
    "# Deep learning and neural networks\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Data handling for deep learning\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Displaying model information\n",
    "from torchsummary import summary\n",
    "\n",
    "# Image manipulation\n",
    "from PIL import Image\n",
    "\n",
    "# Progress bar for Jupyter Notebooks\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Color space conversions\n",
    "from colour import sRGB_to_XYZ, XYZ_to_Lab, Lab_to_XYZ, XYZ_to_sRGB\n",
    "\n",
    "# Custom evaluation\n",
    "from evaluation import calculate_ssim\n",
    "from evaluation import calculate_colourfulness\n",
    "\n",
    "# Memory management\n",
    "import gc\n",
    "\n",
    "# Handling warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Utils <a class=\"anchor\" id=\"utils\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Device Configuration for PyTorch\n",
    "\n",
    "The script determines and prints the computing device to be used for PyTorch operations based on hardware availability:\n",
    "\n",
    "1. **Determine the Device**: The code checks if a CUDA-enabled GPU is available. If yes, it sets the device to `'cuda'` to enable GPU acceleration. If not, it defaults to `'cpu'`.\n",
    "2. **Print Device Information**: It then prints the chosen device (`'cuda'` or `'cpu'`) to the console. This confirmation is useful for verifying that the intended hardware (GPU or CPU) is set for executing the operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the device to use for PyTorch operations\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Print the selected device which device will be used for PyTorch operations.\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "======================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data preparation process is a crucial step in ensuring the quality and effectiveness of the image colorization model.\n",
    "\n",
    "#### Dataset Description\n",
    "- **Total Images**: 64 images.\n",
    "- **Image Formats**: Each image is available in two versions: grayscale and color.\n",
    "- **Image Resolution**: The images have been processed to a consistent resolution, suitable for model processing.\n",
    "\n",
    "#### Data Split\n",
    "The dataset has been divided into two main subsets for training and testing the model:\n",
    "- **Training Data**: 80% of the total images (51 images) are used for model training. The training data includes a mix of grayscale and color images, allowing the model to learn to recognize and reproduce colors from grayscale images.\n",
    "- **Testing Data**: 20% of the total images (13 images) are used as a test set to evaluate the model's performance. Testing ensures that the model can generalize what it has learned to new, unseen images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "======================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Image Colorization and Color Science Computations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script is designed to handle the image colorization process using deep learning frameworks in PyTorch. It details the steps from dataset preparation, transformations, to model inference and visualization:\n",
    "\n",
    "1. **Color Space Conversion Functions**: Includes a function `hsv_to_rgb` that converts images from the HSV color space to RGB using the torch and colour libraries. This function processes a batch of images for display or further processing.\n",
    "\n",
    "2. **Image Transformations**: Defines transformations to resize and normalize images. These transformations prepare images for consistent processing and improve model training efficiency.\n",
    "\n",
    "3. **Custom Dataset Class**: `ImageColorizeDataset` is a PyTorch dataset class that handles loading and preprocessing of images. It splits images into input and output parts, applies transformations, and adjusts the data format for model processing.\n",
    "\n",
    "4. **Data Loaders**: Set up for both training and testing, facilitating the easy batching of data for processing by the model.\n",
    "\n",
    "5. **Model Training and Inference Setup**: Orchestrates the flow from loading data, applying transformations, making predictions with the model, and preparing the images for visualization.\n",
    "\n",
    "6. **Visualization**: Displays a grid of original and colorized images using matplotlib, demonstrating the effectiveness of the model in a visual format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HSV to RGB Conversion Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hsv_to_rgb(H, S, V, device):\n",
    "    \"\"\"\n",
    "    Takes a batch of images\n",
    "    \"\"\"\n",
    "    H = H * 255  # Scale the H component from 0-1 to 0-255\n",
    "    S = S * 255  # Scale the S component from 0-1 to 0-255\n",
    "    V = V * 255  # Scale the V component from 0-1 to 0-255\n",
    "    HSV = torch.cat([H, S, V], dim=1).detach().cpu().numpy()  # Combine H, S, V\n",
    "    rgb_imgs = []  # Initialize a list to store the resulting RGB images\n",
    "    for img in HSV:\n",
    "        rgb_img = img.convert(mode=\"RGB\")  # Convert HSV to RGB\n",
    "        rgb_imgs.append(rgb_img)  # Append the RGB image to the list\n",
    "    return torch.tensor(np.stack(rgb_imgs, axis=0)).to(device)  # Return the images as a PyTorch tensor and move to the specified device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image Data Transformations\n",
    "\n",
    "The image data transformation process involves setting the size and normalizing to prepare images for model training processes:\n",
    "\n",
    "1. **Size Adjustment**: Images are resized to 256 pixels in width and 128 pixels in height. The use of antialiasing helps to reduce moire effects and maintain clarity of the images after resizing.\n",
    "\n",
    "2. **Normalization**: Images are normalized with a mean of 0 and a standard deviation of 0.5. This normalization ensures that the image data has a uniform range of values, which is important for the stability and efficiency of deep learning model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the desired image size\n",
    "image_size = (256, 128)\n",
    "\n",
    "# Compose a series of transformations to be applied to the images\n",
    "t = transforms.Compose([\n",
    "    transforms.Resize(image_size, antialias=True),  # Resize the image with antialias to reduce distortion\n",
    "    transforms.Normalize(mean=0, std=0.5)           # Normalize the images to facilitate neural network processing\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageColorizeDataset(Dataset):\n",
    "    def __init__(self, path: str, device='cpu', train: bool = False, transforms = None):\n",
    "        # Set the dataset mode based on the train argument, 'train' for training and 'test' for testing\n",
    "        _mode = 'train' if train else 'test'\n",
    "        \n",
    "        self.device = device  # Device where data will be processed (CPU or GPU)\n",
    "        self._input_path = os.path.join(path, f'{_mode}_color')  # Path to the data\n",
    "        \n",
    "        self.data = os.listdir(self._input_path)  # List all image files in the directory\n",
    "        \n",
    "        self.transforms = transforms  # Transformations to be applied to the images\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)  # Return the number of data in the dataset\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        to_tensor = transforms.ToTensor()  # Transform the image to a tensor\n",
    "        \n",
    "        item = self.data[idx]  # Retrieve the data item by index\n",
    "        \n",
    "        input_ = cv2.imread(os.path.join(self._input_path, item))  # Open image\n",
    "\n",
    "        height, width, channels = input_.shape\n",
    "        input_ = input_[int(height/2):height, 0:width]  # Crop the image\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            input_ = self.transforms(input_)  # Apply transformations if any\n",
    "        \n",
    "        gray_img = cv2.cvtColor(np.array(input_), cv2.COLOR_BGR2GRAY)  # Convert RGB to Grayscale\n",
    "        hsv_img = cv2.cvtColor(np.array(input_), cv2.COLOR_BGR2HSV)  # Convert RGB to HSV\n",
    "        # img = input_.permute(1, 2, 0).numpy()  # Rearrange axes for color space conversion\n",
    "        # img = Image.fromarray(np.uint8(img))  # Convert RGB to HSV\n",
    "        # img = np.array(img)\n",
    "\n",
    "        gray_img = torch.tensor(gray_img / 255)  # Normalize\n",
    "        hsv_img = torch.tensor(hsv_img / 255)  # Normalize\n",
    "        \n",
    "        return gray_img.to(device), hsv_img.to(device)  # Return as tensor and send to the specified device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image Colorization Dataset Class\n",
    "\n",
    "The `ImageColorizeDataset` class is a part of the PyTorch library that manages image datasets for colorization processes. The class loads and processes images in two modes: training and testing, depending on the argument provided during initialization.\n",
    "\n",
    "##### Initialization\n",
    "- **Path**: The location of the dataset directory containing the images.\n",
    "- **Device**: The device (`cpu` or `gpu`) where data will be processed.\n",
    "- **Train**: Indicates whether the dataset is for training or testing purposes.\n",
    "- **Transforms**: Transformations to be applied to images before processing.\n",
    "\n",
    "##### Loading Data\n",
    "- **`__len__`**: Returns the number of images in the dataset.\n",
    "- **`__getitem__`**: Retrieves an image by index, converts it into a tensor, applies transformations (if any), and converts the color space from sRGB to HSV.\n",
    "\n",
    "##### Transformation Process\n",
    "Transformations on images include resizing, normalizing, and converting color spaces. This process is crucial to prepare the data to meet the requirements of machine learning models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the dataset for training with specified transformations and in training mode\n",
    "train_data = ImageColorizeDataset(\"dataset\", \n",
    "\t\t\t\t\t\t\t\t  transforms=t, train=True)\n",
    "# DataLoader for the training dataset, with a batch size of 8 and shuffling enabled\n",
    "train_dl = DataLoader(train_data, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the dataset for testing with specified transformations and not in training mode\n",
    "test_data = ImageColorizeDataset(\"dataset\", \n",
    "\t\t\t\t\t\t\t\t transforms=t, train=False)\n",
    "# DataLoader for the test dataset, with a batch size of 8 and shuffling enabled\n",
    "test_dl = DataLoader(test_data, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting Up Data Loaders\n",
    "\n",
    "This section of the code initializes and configures data loaders for both training and testing phases of an image colorization model using PyTorch.\n",
    "\n",
    "##### Test Data Loader\n",
    "- **Initialization**: The `ImageColorizeDataset` class is instantiated with the path `\"dataset1\"`, with the transformations defined by `t` and set to `train=False` to indicate that this dataset is for testing purposes.\n",
    "- **Data Loader**: The `DataLoader` is used to create an iterable over the test dataset. It is configured with a batch size of 8 and shuffling enabled to ensure that the model does not see the same sequence of images during testing, which helps simulate a more realistic testing environment.\n",
    "\n",
    "##### Training Data Loader\n",
    "- **Initialization**: Similarly, the `ImageColorizeDataset` is also instantiated for training, with the same path and transformations but with `train=True` to denote that this dataset will be used for training the model.\n",
    "- **Data Loader**: The `DataLoader` for training is similarly configured with a batch size of 8 and shuffling enabled. Shuffling the data during training is crucial as it prevents the model from learning the order of the dataset, thus improving the generalization capabilities of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the next batch of images and targets from the training DataLoader\n",
    "images, targets = next(iter(train_dl))\n",
    "\n",
    "# Convert HSV images back to RGB using the hsv_to_rgb function for visualization\n",
    "to_show = hsv_to_rgb(images, targets, device=device)\n",
    "\n",
    "# Create a grid of images combining the original and colorized images for display\n",
    "grid = torchvision.utils.make_grid(\n",
    "    torch.cat([images.expand(to_show.shape), to_show], dim=0),  # Concatenate original and colorized images\n",
    "    nrow=8,          # Number of images per row\n",
    "    padding=4,       # Space between images\n",
    "    scale_each=True, # Scale images individually\n",
    ")\n",
    "\n",
    "# Plot the grid of images using matplotlib\n",
    "fig = plt.figure(figsize=(16,8))  # Set the figure size\n",
    "plt.imshow(grid.cpu())  # Convert tensor to numpy array and display as image\n",
    "plt.axis('off')  # Hide axes\n",
    "plt.show()  # Display the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing the Results of Image Colorization\n",
    "\n",
    "This section of the code demonstrates how to visualize the results of the image colorization model by combining the original (LAB) and RGB converted images.\n",
    "\n",
    "##### Step-by-Step Explanation:\n",
    "1. **Data Retrieval**: First, a batch of images and their corresponding color targets (LAB values) is retrieved from the training data loader using `next(iter(train_dl))`.\n",
    "\n",
    "2. **Color Space Conversion**: The `lab_to_rgb` function is called to convert these LAB images back into the RGB color space. This is essential for visual inspection, as RGB is the standard format for displaying images on most devices.\n",
    "\n",
    "3. **Image Grid Creation**:\n",
    "   - The original LAB images (converted for display) and the RGB images are concatenated to form a combined tensor.\n",
    "   - `torchvision.utils.make_grid` is utilized to arrange these images into a grid format, making it easier to compare the original and colorized versions side by side.\n",
    "\n",
    "4. **Visualization**:\n",
    "   - A figure of size 16x8 inches is created using `matplotlib`.\n",
    "   - The image grid is displayed using `plt.imshow`, with the axes turned off to emphasize the images themselves.\n",
    "   - The grid displays the original and RGB images in sequence, facilitating a direct visual comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generator <a class=\"anchor\" id=\"generator\"></a>\n",
    "\n",
    "UNet model was used as a generator. It takes image as an input. To randomize output of generator, dropout layers applied both at training and evalutaion time as a noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolutional Block with Residual Connection\n",
    "\n",
    "The `ConvBlock` class extends `nn.Module` and implements a convolutional block with a residual connection, typically used in modern deep learning architectures like ResNets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, norm_layer=nn.BatchNorm2d):\n",
    "        super().__init__()\n",
    "        # Defines a sequential container for two convolutional blocks with BatchNorm and ReLU activation\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, stride=stride, bias=False),\n",
    "            norm_layer(out_channels),  # Normalization layer, here using BatchNorm\n",
    "            nn.ReLU(inplace=True),     # ReLU activation with in-place operation to save memory\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, stride=1, bias=False),\n",
    "            norm_layer(out_channels),  # Second normalization layer\n",
    "            nn.ReLU(inplace=True)      # Second ReLU activation\n",
    "        )\n",
    "        \n",
    "        # Identity mapping that may be used to match dimensions for the residual connection\n",
    "        self.identity = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, stride=stride, bias=False)\n",
    "        self.relu = nn.ReLU(inplace=True)  # Final ReLU activation after adding the residual\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_ = x.detach().clone()  # Detach and clone the input to prevent modifications during forwarding\n",
    "        x_ = self.block(x_)       # Pass the input through the convolutional block\n",
    "        \n",
    "        residual = self.identity(x)  # Apply the identity mapping to the original input\n",
    "        \n",
    "        out = x_ + residual          # Add the output of the convolutional block to the identity mapping\n",
    "        \n",
    "        return self.relu(out)        # Apply a ReLU activation to the combined output and return it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder and Decoder Blocks in a Neural Network\n",
    "\n",
    "The `EncoderBlock` and `DecoderBlock` are fundamental components used in the architecture of autoencoders and U-Nets, typically used in tasks like image segmentation. \n",
    "The Encoder and Decoder blocks are crucial for building deep learning models that require precise spatial transformations, such as in image segmentation where detailed spatial resolution is necessary for accurate pixel-level predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, in_chans, out_chans, sampling_factor=2):\n",
    "        super().__init__()\n",
    "        # Sequential container for an encoder block that includes a max pooling followed by a convolutional block\n",
    "        self.block = nn.Sequential(\n",
    "            nn.MaxPool2d(sampling_factor),  # Reduces the spatial dimensions of the input\n",
    "            ConvBlock(in_chans, out_chans) # Applies a convolutional block to further process the data\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Forward pass of the encoder block: applies pooling and then convolution\n",
    "        return self.block(x)\n",
    "    \n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_chans, out_chans, sampling_factor=2):\n",
    "        super().__init__()\n",
    "        # Upsampling layer to increase the spatial dimensions of the input\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
    "        \n",
    "        # Convolutional block that processes the concatenated input from the upsampled feature map and the skip connection\n",
    "        self.block = ConvBlock(in_chans + out_chans, out_chans)\n",
    "        \n",
    "    def forward(self, x, skip):\n",
    "        # Upsamples the input feature map\n",
    "        x = self.upsample(x)\n",
    "        # Concatenates the upsampled feature map with the skip connection feature map\n",
    "        x = torch.cat([x, skip], dim=1)\n",
    "        # Processes the concatenated feature maps using a convolutional block\n",
    "        x = self.block(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### U-Net Architecture Implementation\n",
    "\n",
    "The `UNet` class implements a U-Net architecture, a popular model for biomedical image segmentation that features a symmetric encoder-decoder structure.\n",
    "The U-Net architecture effectively combines high-level feature extraction with precise localization, facilitated by skip connections that help in transferring spatial information to the decoder. This structure makes it particularly effective for tasks where precise segmentation is crucial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=2, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        # Initialize the encoder part of the U-Net with progressively increasing channels\n",
    "        self.encoder = nn.ModuleList([\n",
    "            ConvBlock(in_channels, 64),\n",
    "            EncoderBlock(64, 128),\n",
    "            EncoderBlock(128, 256),\n",
    "            EncoderBlock(256, 512),\n",
    "        ])\n",
    "        # Initialize the decoder part of the U-Net with progressively decreasing channels\n",
    "        self.decoder = nn.ModuleList([\n",
    "            DecoderBlock(512, 256),\n",
    "            DecoderBlock(256, 128),\n",
    "            DecoderBlock(128, 64)\n",
    "        ])\n",
    "        # Dropout layer to prevent overfitting\n",
    "        self.dropout = nn.Dropout2d(dropout_rate)\n",
    "        # Final convolution layer to map the decoded features to the desired number of output channels\n",
    "        self.logits = nn.Conv2d(in_channels=64, out_channels=out_channels, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded = []\n",
    "        # Pass input through each encoder block, apply dropout, and store intermediate outputs for skip connections\n",
    "        for enc in self.encoder:\n",
    "            x = enc(x)\n",
    "            x = self.dropout(x)\n",
    "            encoded.append(x)\n",
    "\n",
    "        enc_out = encoded.pop()\n",
    "        \n",
    "        # Start the decoding process using the stored encoded features\n",
    "        for dec in self.decoder:\n",
    "            enc_out = encoded.pop()  # Retrieve the corresponding encoder output for skip connections\n",
    "            x = dec(x, enc_out)  # Decoder block processes input with skip connections\n",
    "        # Apply a sigmoid activation to the final layer's output to normalize the output to [0,1] range\n",
    "        return F.sigmoid(self.logits(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trigger the garbage collector to free memory from unreferenced objects.\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Discriminator <a class=\"anchor\" id=\"discriminator\"></a>\n",
    "\n",
    "Due to our input image shape `batch_size x 3 x 256 x 128`, in out PatchGAN discriminator we have 3 sequential `3 x 3` conv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PatchGAN Network Architecture\n",
    "\n",
    "The `PatchGAN` class defines a convolutional neural network specifically designed for processing patches of images, commonly used in tasks such as image segmentation and texture synthesis. This model architecture is particularly effective in applications where local texture patterns are crucial, such as in style transfer and local image editing.\n",
    "PatchGAN can be used in various applications within computer vision, particularly in generative adversarial networks (GANs) where the model needs to discriminate between real and synthetic images at the scale of image patches, thus enabling detailed texture and pattern discrimination.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchGAN(nn.Module):\n",
    "    def __init__(self, in_channels, n_features=64, n_layers=3):\n",
    "        super().__init__()\n",
    "        # Initialize the PatchGAN architecture parameters\n",
    "        k_size = 4  # kernel size\n",
    "        p_size = 2  # padding size\n",
    "        \n",
    "        # Starting layer configuration with a convolutional layer and a LeakyReLU activation\n",
    "        seq = [\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=n_features, kernel_size=k_size, padding=p_size, stride=2),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        ]\n",
    "        \n",
    "        # Factor multipliers for feature map scaling in subsequent layers\n",
    "        f_mult = 1\n",
    "        f_mult_prev = 1\n",
    "        \n",
    "        # Construct additional convolutional layers as specified by n_layers\n",
    "        for i in range(1, n_layers):\n",
    "            f_mult_prev = f_mult\n",
    "            f_mult = min(2 ** i, 8)  # Update feature multiplier with constraints\n",
    "            \n",
    "            seq.append(nn.Conv2d(in_channels=f_mult_prev * n_features, out_channels=f_mult * n_features, kernel_size=k_size, padding=p_size, stride=2))\n",
    "            seq.append(nn.BatchNorm2d(f_mult * n_features))  # Batch normalization for stability\n",
    "            seq.append(nn.LeakyReLU(0.2, True))  # LeakyReLU for non-linear processing\n",
    "\n",
    "        # Last convolutional layers before the final output layer\n",
    "        f_mult_prev = f_mult\n",
    "        f_mult = min(2 ** n_layers, 8)\n",
    "        \n",
    "        seq += [\n",
    "            nn.Conv2d(n_features * f_mult_prev, n_features * f_mult, kernel_size=k_size, stride=1, padding=p_size),\n",
    "            nn.BatchNorm2d(n_features * f_mult),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        ]\n",
    "\n",
    "        # Final convolutional layer to produce a 1-channel prediction map\n",
    "        seq += [nn.Conv2d(n_features * f_mult, 1, kernel_size=k_size, stride=1, padding=p_size)]\n",
    "        self.model = nn.Sequential(*seq)\n",
    "\n",
    "    def forward(self, x, label):\n",
    "        # Concatenate input and label along the channel dimension for conditional GAN processing\n",
    "        x = torch.cat((x, label), dim=1)\n",
    "        x = self.model(x)  # Pass through the network\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = PatchGAN(in_channels=3).to(device)  # Initialize the PatchGAN discriminator\n",
    "\n",
    "# Display the model architecture\n",
    "summary(D, [(1, image_size[0], image_size[1]), (2, image_size[0], image_size[1])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PatchGAN Model\n",
    "\n",
    "The `PatchGAN` model is initialized with three input channels and configured to process images such as those in RGB format. The model is then moved to a computational device (CPU or GPU) for efficient processing. Here is an explanation of each step:\n",
    "\n",
    "##### Model Initialization and Device Allocation:\n",
    "- **PatchGAN(in_channels=3)**: Constructs a `PatchGAN` model that accepts images with three channels (RGB). This model is particularly used in applications like image synthesis or segmentation where understanding local patches is crucial.\n",
    "- **.to(device)**: This method transfers the model to a specified device (`cpu` or `gpu`). This is essential for leveraging hardware acceleration during training or inference, enhancing performance and efficiency.\n",
    "\n",
    "##### Model Summary with torchsummary:\n",
    "- **summary(D, [(1, image_size[0], image_size[1]), (2, image_size[0], image_size[1])])**: Provides a detailed summary of the model. The summary includes layer-by-layer details, output shapes, and the number of parameters at each stage.\n",
    "  - The parameters for the `summary` function indicate the size of the input tensors:\n",
    "    - The first input tensor, with 1 channel and dimensions defined by `image_size`, could represent a specific feature map or modality.\n",
    "    - The second input tensor, with 2 channels and the same spatial dimensions, might carry additional contextual or conditional information influencing the network's predictions.\n",
    "\n",
    "#### Practical Application:\n",
    "This initialization and summary are particularly valuable for developers and researchers:\n",
    "- **Model Development**: Understanding the internal architecture, including the flow and transformation of data through the network, is crucial during the development and debugging phases.\n",
    "- **Resource Management**: Insight into parameter count and memory usage helps in optimizing the model for different computational environments, ensuring efficient deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = UNet(in_channels=1, out_channels=2).to(device)\n",
    "# G2 = UNet(in_channels=2, out_channels=1).to(device)\n",
    "\n",
    "# summary(G, (1, 384, 384))\n",
    "summary(G, (1, image_size[0], image_size[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Trainer <a class=\"anchor\" id=\"training\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer: \n",
    "    def __init__(self, G, D, device, batch_size = 4, lr=1e-3, discriminator_to_generator_training_rate = 2, plot_rate=1):\n",
    "        \n",
    "        self.G = G\n",
    "        self.D = D\n",
    "        \n",
    "        self.L1_G_loss = nn.L1Loss()\n",
    "        self.G_loss = nn.BCEWithLogitsLoss()\n",
    "        self.D_loss = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "        self.G_optim = torch.optim.Adam(params=G.parameters(), lr=lr)\n",
    "        self.D_optim = torch.optim.Adam(params=D.parameters(), lr=4e-4)\n",
    "\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.loss_G_per_epoch = []\n",
    "        self.loss_D_per_epoch = []\n",
    "        self.loss_D_real_per_epoch = []\n",
    "        self.loss_D_fake_per_epoch = []\n",
    "        \n",
    "        self.loss_G_history = []\n",
    "        self.loss_D_history = []\n",
    "        self.loss_D_real_history = []\n",
    "        self.loss_D_fake_history = []\n",
    "\n",
    "        self.ssim_history = []\n",
    "        self.colourfulness_history = []\n",
    "        \n",
    "        self.k = discriminator_to_generator_training_rate\n",
    "        self.device = device\n",
    "        \n",
    "        self.plot_rate = plot_rate\n",
    "    \n",
    "    def train(self, dataloader, epoch):\n",
    "        for epoch in range(epoch):\n",
    "            self.loss_G_history.append([])\n",
    "            self.loss_D_history.append([])\n",
    "            self.loss_D_real_history.append([])\n",
    "            self.loss_D_fake_history.append([])\n",
    "            \n",
    "            print(f'EPOCH: {epoch + 1}')\n",
    "            for i, (images, targets) in enumerate(tqdm(dataloader)):\n",
    "                self._train_discriminator(images, targets)\n",
    "                \n",
    "                if (i + 1) % self.k == 0:\n",
    "                    self._train_generator(images, targets)\n",
    "    \n",
    "            self.loss_G_per_epoch.append(np.mean(self.loss_G_history[-1]))\n",
    "            self.loss_D_per_epoch.append(np.mean(self.loss_D_history[-1]))\n",
    "            self.loss_D_real_per_epoch.append(np.mean(self.loss_D_real_history[-1]))\n",
    "            self.loss_D_fake_per_epoch.append(np.mean(self.loss_D_fake_history[-1]))\n",
    "\n",
    "            # Calculate SSIM and Colourfulness for the epoch\n",
    "            self.ssim_history.append(self._calculate_epoch_ssim(dataloader))\n",
    "            self.colourfulness_history.append(self._calculate_epoch_colourfulness(dataloader))\n",
    "            \n",
    "            if (epoch + 1) % self.plot_rate == 0:\n",
    "                self._plot_epoch_stats(epoch)\n",
    "                self._plot_fake_images(images, targets)\n",
    "            self._plot_stats()\n",
    "\n",
    "    def _calculate_epoch_ssim(self, dataloader):\n",
    "        self.G.eval()\n",
    "        ssim_total = 0\n",
    "        count = 0\n",
    "        with torch.no_grad():\n",
    "            for images, targets in dataloader:\n",
    "                images, targets = images.to(self.device), targets.to(self.device)\n",
    "                fake = self.G(images)\n",
    "                ssim_total += calculate_ssim(images, fake, targets)\n",
    "                count += 1\n",
    "        return ssim_total / count\n",
    "\n",
    "    def _calculate_epoch_colourfulness(self, dataloader):\n",
    "        self.G.eval()\n",
    "        colourfulness_total = 0\n",
    "        count = 0\n",
    "        with torch.no_grad():\n",
    "            for images, targets in dataloader:\n",
    "                images, targets = images.to(self.device), targets.to(self.device)\n",
    "                fake = self.G(images)\n",
    "                colourfulness_total += calculate_colourfulness(images, fake)\n",
    "                count += 1\n",
    "        return colourfulness_total / count\n",
    "\n",
    "    def _train_generator(self, inputs, targets):\n",
    "        \n",
    "        self.G_optim.zero_grad()\n",
    "        \n",
    "        self.G.train()\n",
    "        self.D.eval()\n",
    "        \n",
    "        l = 1\n",
    "        \n",
    "        fake_targets = self.G(inputs)\n",
    "        \n",
    "        predictions = self.D(inputs, fake_targets)\n",
    "        fake_labels = torch.zeros(*predictions.shape,  device=self.device)\n",
    "        \n",
    "        L1_loss = self.L1_G_loss(fake_targets, targets)\n",
    "        BCE_loss = self.G_loss(predictions, fake_labels)\n",
    "        loss_g = BCE_loss + l * L1_loss\n",
    "        self.loss_G_history[-1].append(loss_g.item())\n",
    "        loss_g.backward()\n",
    "        self.G_optim.step()\n",
    "                \n",
    "    \n",
    "    def _train_discriminator(self, inputs, real_targets):\n",
    "        self.D_optim.zero_grad()\n",
    "        \n",
    "        self.G.eval()\n",
    "        self.D.train()\n",
    "        \n",
    "        # train on real images\n",
    "        \n",
    "        real_predictions = self.D(inputs, real_targets)\n",
    "        real_label = torch.ones(*real_predictions.shape,  device=self.device)\n",
    "\n",
    "        real = self.D_loss(real_predictions, real_label)\n",
    "        \n",
    "        \n",
    "        # train on fake images\n",
    "                \n",
    "        fake_targets = self.G(inputs)\n",
    "        fake_predictions = self.D(inputs, fake_targets.detach())\n",
    "        fake_label = torch.zeros(*fake_predictions.shape, device=self.device)\n",
    "\n",
    "        fake = self.D_loss(fake_predictions, fake_label)\n",
    "        \n",
    "        loss_d = real + fake\n",
    "        \n",
    "        self.loss_D_history[-1].append(loss_d.item())\n",
    "        self.loss_D_real_history[-1].append(real.item())\n",
    "        self.loss_D_fake_history[-1].append(fake.item())\n",
    "\n",
    "        loss_d.backward()\n",
    "        self.D_optim.step()\n",
    "        \n",
    "    def _plot_fake_images(self, images, targets, nrow=8):\n",
    "        \"\"\"\n",
    "        Showing the generator's results\n",
    "        \"\"\"\n",
    "        \n",
    "        self.G.eval()\n",
    "                \n",
    "        fake = self.G(images)\n",
    "        \n",
    "        to_show = lab_to_rgb(images, fake, device=self.device)\n",
    "        to_show_real = lab_to_rgb(images, targets, device=self.device)\n",
    "\n",
    "        # Calculate SSIM\n",
    "        score_ssim = calculate_ssim(images, fake, targets)\n",
    "        print(f\"SSIM: {score_ssim}\")\n",
    "\n",
    "        # Calculate Colourfulness\n",
    "        score_colourfulness = calculate_colourfulness(images, fake)\n",
    "        print(f\"Colourfulness: {score_colourfulness}\")\n",
    "\n",
    "        a = fake[:, 0, :, :].unsqueeze(1).expand(to_show.shape)\n",
    "        b = fake[:, 1, :, :].unsqueeze(1).expand(to_show.shape)\n",
    "        grid = torchvision.utils.make_grid(torch.cat([images.expand(to_show.shape), a, b, to_show, to_show_real], dim=0), nrow=8, padding=1, scale_each=True)\n",
    "\n",
    "        fig = plt.figure(figsize=(16,8))\n",
    "        plt.imshow(grid.cpu().permute(1,2,0))\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        \n",
    "    def _plot_stats(self):\n",
    "        \"\"\"\n",
    "        Plotting stats of history training\n",
    "        \"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(10, 4))\n",
    "        sns.lineplot(self.loss_D_per_epoch, label=\"discriminator\", ax=axes[0][0])\n",
    "        sns.lineplot(self.loss_G_per_epoch, label=\"generator\", ax=axes[0][1])\n",
    "        \n",
    "        sns.lineplot(self.loss_D_real_per_epoch, label=\"real\", ax=axes[1][0])\n",
    "        sns.lineplot(self.loss_D_fake_per_epoch, label=\"fake\", ax=axes[1][1])\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "    def _plot_epoch_stats(self, epoch):\n",
    "        \"\"\"\n",
    "        Plotting stats of history training\n",
    "        \"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(10, 4))\n",
    "        sns.lineplot(self.loss_D_history[epoch], label=\"discriminator\", ax=axes[0][0])\n",
    "        sns.lineplot(self.loss_G_history[epoch], label=\"generator\", ax=axes[0][1])\n",
    "        \n",
    "        sns.lineplot(self.loss_D_real_history[epoch], label=\"real\", ax=axes[1][0])\n",
    "        sns.lineplot(self.loss_D_fake_history[epoch], label=\"fake\", ax=axes[1][1])\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    def _plot_losses(self):\n",
    "        \"\"\"\n",
    "        Plot the generator and discriminator losses per epoch.\n",
    "        \"\"\"\n",
    "        epochs = range(1, len(self.loss_G_per_epoch) + 1)\n",
    "\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(epochs, self.loss_G_per_epoch, label='Generator loss', color='blue')\n",
    "        plt.plot(epochs, self.loss_D_per_epoch, label='Discriminator loss', color='red')\n",
    "        plt.title('Loss vs. Epoch')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_ssim_and_colourfulness(self):\n",
    "        \"\"\"\n",
    "        Plot SSIM and Colourfulness over epochs.\n",
    "        \"\"\"\n",
    "        epochs = range(1, len(self.ssim_history) + 1)\n",
    "\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(epochs, self.ssim_history, label='SSIM', color='green')\n",
    "        plt.plot(epochs, self.colourfulness_history, label='Colourfulness', color='purple')\n",
    "        plt.title('SSIM and Colourfulness vs. Epoch')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Score')\n",
    "        plt.legend()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trainer Class for GANs\n",
    "\n",
    "##### Key Components and Initialization\n",
    "- **Models and Device**: The trainer initializes with generator (`G`) and discriminator (`D`) models, and specifies the device (`CPU` or `GPU`) for computation.\n",
    "- **Loss Functions**: Utilizes L1 and Binary Cross-Entropy (BCE) losses to measure accuracy and fidelity in generated images.\n",
    "- **Optimizers**: Adam optimizers are employed for both models to iteratively improve through gradient descent.\n",
    "\n",
    "##### Training Process\n",
    "- **Batch and Epoch Management**: Handles data loading and batches through the provided `dataloader`. The training is epoch-based, with detailed loss tracking and periodic updates.\n",
    "- **Discriminator and Generator Training**: Trains the discriminator more frequently than the generator based on a specified rate (`discriminator_to_generator_training_rate`), balancing the training between sharpening discriminator accuracy and enhancing generator quality.\n",
    "- **Loss Tracking and Visualization**: Maintains a comprehensive log of losses for both real and fake inputs to monitor the training progress. Visualizations of losses and generated images are periodically produced to assess the model's performance and convergence.\n",
    "\n",
    "##### Methods for Detailed Operations\n",
    "- **_train_generator() and _train_discriminator()**: Core methods that handle the training of the generator and discriminator, including backpropagation and parameter updates.\n",
    "- **_plot_fake_images() and _plot_stats()**: Utility methods for visual output and statistical analysis, providing insights into the training process and outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measuring Image Quality in Image Generation Tasks\n",
    "\n",
    "In tasks involving image generation, such as those with Generative Adversarial Networks (GANs), it's crucial to quantitatively assess the quality of generated images. Two important metrics are used for this purpose: the Structural Similarity Index (SSIM) and the Colourfulness metric.\n",
    "\n",
    "##### Structural Similarity Index (SSIM)\n",
    "\n",
    "SSIM is a perception-based model that quantifies the similarity between two images. It is used to measure the quality of reconstruction or generation in comparison to an original or ground-truth image.\n",
    "\n",
    "- **Usage**: `calculate_ssim(L, ab_gen, ab_gt)`\n",
    "- **Input**:\n",
    "  - `L`: Lightness component of the LAB color space.\n",
    "  - `ab_gen`: Generated image color components.\n",
    "  - `ab_gt`: Ground truth image color components.\n",
    "- **Process**:\n",
    "  - The function normalizes the L and AB components to standard LAB color space ranges.\n",
    "  - Converts LAB color space images to sRGB for proper visualization.\n",
    "  - Calculates SSIM for each image in the batch and averages the scores.\n",
    "\n",
    "##### Colourfulness Metric\n",
    "\n",
    "Based on a study by Hasler and Suesstrunk, the Colourfulness metric measures the vividness of an image, which can be particularly useful in evaluating the performance of color generation models.\n",
    "\n",
    "- **Usage**: `calculate_colourfulness(L, ab_gen)`\n",
    "- **Input**:\n",
    "  - `L`: Lightness component of the LAB color space.\n",
    "  - `ab_gen`: Generated image color components.\n",
    "- **Process**:\n",
    "  - Normalizes and converts the LAB images to sRGB.\n",
    "  - Computes differences between the red-green and yellow-blue components.\n",
    "  - Calculates mean and standard deviation of these differences to derive the colourfulness score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(G, D, device) # Initialize the trainer with the generator and discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the models for 50 epochs\n",
    "trainer.train(train_dl, epoch=50) \t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.plot_ssim_and_colourfulness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer._plot_losses() \t# Plot the generator and discriminator losses per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.G\t# Set the generator to evaluation mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elraang",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
