{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c45c49e-45b4-4a4d-9a6c-5d9f063ef03d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Baru\\Lib\\site-packages\\colour\\utilities\\verbose.py:265: ColourWarning: \"vaab/colour\" was detected in \"sys.path\", please define a \"COLOUR_SCIENCE__COLOUR__IMPORT_VAAB_COLOUR=True\" environment variable to import its objects into \"colour\" namespace!\n",
      "  warn(*args, **kwargs)  # noqa: B028\n"
     ]
    }
   ],
   "source": [
    "# Operating system and file operations\n",
    "import os\n",
    "\n",
    "# Numerical operations and array processing\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Data visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Random number generation\n",
    "import random\n",
    "\n",
    "# Image processing and computer vision\n",
    "import cv2\n",
    "\n",
    "# Deep learning and neural networks\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Data handling for deep learning\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Displaying model information\n",
    "from torchsummary import summary\n",
    "\n",
    "# Image manipulation\n",
    "from PIL import Image\n",
    "\n",
    "# Type annotations\n",
    "from typing import List\n",
    "\n",
    "# Progress bar for Jupyter Notebooks\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Color space conversions\n",
    "from colour import sRGB_to_XYZ, XYZ_to_Lab, Lab_to_XYZ, XYZ_to_sRGB\n",
    "\n",
    "# Image quality assessment\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "# Memory management\n",
    "import gc\n",
    "\n",
    "# Evaluation\n",
    "from evaluation import calculate_ssim\n",
    "from evaluation import calculate_colourfulness\n",
    "\n",
    "# Handling warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95f3c36e-7bf4-4992-9e34-d35e887972a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Determine the device to use for PyTorch operations\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Print the selected device which device will be used for PyTorch operations.\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5f7a148-aebf-4de4-956e-363f951239f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lab_to_rgb(L, ab, device):\n",
    "    \"\"\"\n",
    "    Takes a batch of images\n",
    "    \"\"\"\n",
    "    L = 100 * L  # Scale the L component from 0-1 to 0-100\n",
    "    ab = (ab - 0.5) * 256  # Adjust the a and b components to the correct range\n",
    "    Lab = torch.cat([L, ab], dim=1).permute(0, 2, 3, 1).detach().cpu().numpy()  # Combine L, a, b, and rearrange the format for processing\n",
    "    rgb_imgs = []  # Initialize a list to store the resulting RGB images\n",
    "    for img in Lab:\n",
    "        img = Lab_to_XYZ(img)  # Convert LAB to XYZ\n",
    "        img = XYZ_to_sRGB(img)  # Convert XYZ to RGB\n",
    "        rgb_imgs.append(img)  # Append the RGB image to the list\n",
    "    return torch.tensor(np.stack(rgb_imgs, axis=0)).permute(0, 3, 1, 2).to(device)  # Return the images as a PyTorch tensor and move to the specified device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f19d7336-d535-43f8-9048-3538d143a93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_file):\n",
    "    loaded_model = pickle.load(open(model_file, 'rb'))\n",
    "    return loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79c8954b-481a-4878-8fe5-0f916f0608a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, norm_layer=nn.BatchNorm2d):\n",
    "        super().__init__()\n",
    "        # Defines a sequential container for two convolutional blocks with BatchNorm and ReLU activation\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, stride=stride, bias=False),\n",
    "            norm_layer(out_channels),  # Normalization layer, here using BatchNorm\n",
    "            nn.ReLU(inplace=True),     # ReLU activation with in-place operation to save memory\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, stride=1, bias=False),\n",
    "            norm_layer(out_channels),  # Second normalization layer\n",
    "            nn.ReLU(inplace=True)      # Second ReLU activation\n",
    "        )\n",
    "        \n",
    "        # Identity mapping that may be used to match dimensions for the residual connection\n",
    "        self.identity = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, stride=stride, bias=False)\n",
    "        self.relu = nn.ReLU(inplace=True)  # Final ReLU activation after adding the residual\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_ = x.detach().clone()  # Detach and clone the input to prevent modifications during forwarding\n",
    "        x_ = self.block(x_)       # Pass the input through the convolutional block\n",
    "        \n",
    "        residual = self.identity(x)  # Apply the identity mapping to the original input\n",
    "        \n",
    "        out = x_ + residual          # Add the output of the convolutional block to the identity mapping\n",
    "        \n",
    "        return self.relu(out)        # Apply a ReLU activation to the combined output and return it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbc7067c-775e-4e78-bda5-111b593d7951",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, in_chans, out_chans, sampling_factor=2):\n",
    "        super().__init__()\n",
    "        # Sequential container for an encoder block that includes a max pooling followed by a convolutional block\n",
    "        self.block = nn.Sequential(\n",
    "            nn.MaxPool2d(sampling_factor),  # Reduces the spatial dimensions of the input\n",
    "            ConvBlock(in_chans, out_chans) # Applies a convolutional block to further process the data\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Forward pass of the encoder block: applies pooling and then convolution\n",
    "        return self.block(x)\n",
    "    \n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_chans, out_chans, sampling_factor=2):\n",
    "        super().__init__()\n",
    "        # Upsampling layer to increase the spatial dimensions of the input\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
    "        \n",
    "        # Convolutional block that processes the concatenated input from the upsampled feature map and the skip connection\n",
    "        self.block = ConvBlock(in_chans + out_chans, out_chans)\n",
    "        \n",
    "    def forward(self, x, skip):\n",
    "        # Upsamples the input feature map\n",
    "        x = self.upsample(x)\n",
    "        # Concatenates the upsampled feature map with the skip connection feature map\n",
    "        x = torch.cat([x, skip], dim=1)\n",
    "        # Processes the concatenated feature maps using a convolutional block\n",
    "        x = self.block(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b7f9e69-3c5f-4d5c-b876-d69d425ef2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=2, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        # Initialize the encoder part of the U-Net with progressively increasing channels\n",
    "        self.encoder = nn.ModuleList([\n",
    "            ConvBlock(in_channels, 64),\n",
    "            EncoderBlock(64, 128),\n",
    "            EncoderBlock(128, 256),\n",
    "            EncoderBlock(256, 512),\n",
    "        ])\n",
    "        # Initialize the decoder part of the U-Net with progressively decreasing channels\n",
    "        self.decoder = nn.ModuleList([\n",
    "            DecoderBlock(512, 256),\n",
    "            DecoderBlock(256, 128),\n",
    "            DecoderBlock(128, 64)\n",
    "        ])\n",
    "        # Dropout layer to prevent overfitting\n",
    "        self.dropout = nn.Dropout2d(dropout_rate)\n",
    "        # Final convolution layer to map the decoded features to the desired number of output channels\n",
    "        self.logits = nn.Conv2d(in_channels=64, out_channels=out_channels, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded = []\n",
    "        # Pass input through each encoder block, apply dropout, and store intermediate outputs for skip connections\n",
    "        for enc in self.encoder:\n",
    "            x = enc(x)\n",
    "            x = self.dropout(x)\n",
    "            encoded.append(x)\n",
    "\n",
    "        enc_out = encoded.pop()\n",
    "        \n",
    "        # Start the decoding process using the stored encoded features\n",
    "        for dec in self.decoder:\n",
    "            enc_out = encoded.pop()  # Retrieve the corresponding encoder output for skip connections\n",
    "            x = dec(x, enc_out)  # Decoder block processes input with skip connections\n",
    "        # Apply a sigmoid activation to the final layer's output to normalize the output to [0,1] range\n",
    "        return F.sigmoid(self.logits(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90b4ac77-24c4-452d-8c37-74f43e5e4b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def colorize_images_in_folder(model, input_folder, output_folder, device='cuda'):\n",
    "    # Create output folder if it doesn't exist\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Define transforms\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 128), antialias=True),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=0, std=0.5)\n",
    "    ])\n",
    "\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Get list of image files in the input folder\n",
    "    image_files = [f for f in os.listdir(input_folder) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "    # Process each image\n",
    "    for image_file in tqdm(image_files, desc=\"Colorizing images\"):\n",
    "        input_path = os.path.join(input_folder, image_file)\n",
    "        output_path = os.path.join(output_folder, f\"colorized_{image_file}\")\n",
    "\n",
    "        # Load and preprocess the image\n",
    "        input_image = Image.open(input_path).convert('L')  # Convert to grayscale\n",
    "        w, h = input_image.size\n",
    "        input_image = input_image.crop((0, 0, w, h//2))  # Crop top half as per your dataset\n",
    "\n",
    "        # Apply transforms\n",
    "        input_tensor = transform(input_image).unsqueeze(0).to(device)\n",
    "\n",
    "        # Generate colorized output\n",
    "        with torch.no_grad():\n",
    "            output = model(input_tensor)\n",
    "\n",
    "        # Convert output to RGB\n",
    "        L = input_tensor * 100\n",
    "        ab = (output - 0.5) * 256\n",
    "        Lab = torch.cat([L, ab], dim=1).squeeze().permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "        # Convert LAB to RGB\n",
    "        rgb_image = XYZ_to_sRGB(Lab_to_XYZ(Lab))\n",
    "        \n",
    "        # Clip values to [0, 1] range and convert to uint8\n",
    "        rgb_image = np.clip(rgb_image, 0, 1)\n",
    "        rgb_image = (rgb_image * 255).astype(np.uint8)\n",
    "\n",
    "        # Convert to PIL Image and save\n",
    "        colorized_image = Image.fromarray(rgb_image)\n",
    "        colorized_image.save(output_path)\n",
    "\n",
    "    print(f\"Colorized images saved in {output_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "534bdcf4-9700-4c6c-b1fd-b64a505aa727",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = load_model('gen_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cde492cd-ed4d-4e65-8a48-02c7c2985801",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73878df15eb0492c9a92d2259c5546b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Colorizing images:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colorized images saved in output55/\n"
     ]
    }
   ],
   "source": [
    "Generated = colorize_images_in_folder(G, \"dataset/test_black\", \"output55/\", device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
