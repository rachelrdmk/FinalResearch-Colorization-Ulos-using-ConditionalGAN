{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cd0bb61",
   "metadata": {},
   "source": [
    "### 1. Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0548c582",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import sys\n",
    "import shutil\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.layers import Input, Dense, Flatten, Dropout, Reshape, Concatenate\n",
    "from keras.layers import BatchNormalization, Activation, Conv2D, Conv2DTranspose, UpSampling2D\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed1b4b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "time_start = time.time()\n",
    "\n",
    "def sec_to_time(seconds):\n",
    "    days = seconds // (60 * 60 * 24)\n",
    "    seconds -= days * (60 * 60 * 24)\n",
    "    hours = seconds // (60 * 60)\n",
    "    seconds -= hours * (60 * 60)\n",
    "    minutes = seconds // 60\n",
    "    seconds -= minutes * 60\n",
    "    return f\"{int(days)}:{int(hours):02}:{int(minutes):02}:{int(seconds):02}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d5c3f8",
   "metadata": {},
   "source": [
    "### 2. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "677ccd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_dataset_dir = 'Dataset'\n",
    "gray_dataset_dir = 'Output Grayscale'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "890c2623",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_directory(dir_a, dir_b):\n",
    "    list_path_color_images = []\n",
    "    list_path_gray_images = []\n",
    "    list_name = []\n",
    "    \n",
    "    for root, dirs, files in os.walk(dir_a):\n",
    "        for file in files:\n",
    "            if file.endswith('.jpg') or file.endswith('.jpeg') or file.endswith('.png'):\n",
    "                list_path_color_images.append(root+'/'+file)\n",
    "                list_name.append(re.split(' |_', file)[0].lower())\n",
    "                \n",
    "    for root, dirs, files in os.walk(dir_b):\n",
    "        for file in files:\n",
    "            if (file.endswith('.jpg') or file.endswith('.jpeg') or file.endswith('.png')):\n",
    "                list_path_gray_images.append(root+'/'+file)\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'ulos_type': list_name,\n",
    "        'path_color': list_path_color_images,\n",
    "        'path_gray': list_path_gray_images\n",
    "    })\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fc0ca5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ulos_type</th>\n",
       "      <th>path_color</th>\n",
       "      <th>path_gray</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>harungguan</td>\n",
       "      <td>Dataset\\harungguan/Harungguan Biru_Ruth Theres...</td>\n",
       "      <td>Output Grayscale\\harungguan/Harungguan Biru_Ru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>harungguan</td>\n",
       "      <td>Dataset\\harungguan/Harungguan Coklat Terang_Ru...</td>\n",
       "      <td>Output Grayscale\\harungguan/Harungguan Coklat ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>harungguan</td>\n",
       "      <td>Dataset\\harungguan/Harungguan Hijau_Ruth There...</td>\n",
       "      <td>Output Grayscale\\harungguan/Harungguan Hijau_R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>harungguan</td>\n",
       "      <td>Dataset\\harungguan/Harungguan Merah_Ruth There...</td>\n",
       "      <td>Output Grayscale\\harungguan/Harungguan Merah_R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>harungguan</td>\n",
       "      <td>Dataset\\harungguan/Harungguan Orange_Ruth Ther...</td>\n",
       "      <td>Output Grayscale\\harungguan/Harungguan Orange_...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ulos_type                                         path_color  \\\n",
       "0  harungguan  Dataset\\harungguan/Harungguan Biru_Ruth Theres...   \n",
       "1  harungguan  Dataset\\harungguan/Harungguan Coklat Terang_Ru...   \n",
       "2  harungguan  Dataset\\harungguan/Harungguan Hijau_Ruth There...   \n",
       "3  harungguan  Dataset\\harungguan/Harungguan Merah_Ruth There...   \n",
       "4  harungguan  Dataset\\harungguan/Harungguan Orange_Ruth Ther...   \n",
       "\n",
       "                                           path_gray  \n",
       "0  Output Grayscale\\harungguan/Harungguan Biru_Ru...  \n",
       "1  Output Grayscale\\harungguan/Harungguan Coklat ...  \n",
       "2  Output Grayscale\\harungguan/Harungguan Hijau_R...  \n",
       "3  Output Grayscale\\harungguan/Harungguan Merah_R...  \n",
       "4  Output Grayscale\\harungguan/Harungguan Orange_...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = read_directory(color_dataset_dir, gray_dataset_dir)\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d716ab",
   "metadata": {},
   "source": [
    "### 3. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42750401",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_resize(original_image, w = 360, h = 900):\n",
    "    shrinked_image = cv2.resize(original_image, (w, h),\n",
    "                                interpolation = cv2.INTER_AREA)\n",
    "    \n",
    "    # DEBUG\n",
    "    # plt.imshow(shrinked_image)\n",
    "    # plt.axis(\"off\")\n",
    "    # plt.show()\n",
    "\n",
    "    return shrinked_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa168af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_quantization(original_image, n_clusters = 12):\n",
    "    # flatten image\n",
    "    reshaped_image = np.reshape(original_image,\n",
    "                                ((original_image.shape[0] * original_image.shape[1]), 3))\n",
    "\n",
    "    model = KMeans(n_clusters=n_clusters)\n",
    "    target = model.fit_predict(reshaped_image)\n",
    "    color_space = model.cluster_centers_\n",
    "    \n",
    "    output_image = np.reshape(color_space.astype(\"uint8\")[target],\n",
    "                              (original_image.shape[0],\n",
    "                               original_image.shape[1], 3))\n",
    "    \n",
    "    quantized_image = cv2.cvtColor(output_image, cv2.COLOR_BGR2Lab)\n",
    "\n",
    "    # DEBUG\n",
    "    # plt.imshow(quantized_image)\n",
    "    # plt.axis(\"off\")\n",
    "    # plt.show()\n",
    "    \n",
    "    return quantized_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5205be6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in df.itertuples():\n",
    "    color_image = cv2.imread(i.path_color)\n",
    "    gray_image = cv2.imread(i.path_gray)\n",
    "    \n",
    "    # preprocessing\n",
    "    color_image = image_resize(color_image)\n",
    "    color_image = image_quantization(color_image)\n",
    "    \n",
    "    gray_image = image_resize(gray_image)\n",
    "    gray_image = image_quantization(gray_image)\n",
    "    gray_image = cv2.cvtColor(gray_image, cv2.COLOR_Lab2RGB)\n",
    "    gray_image = cv2.cvtColor(gray_image, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    # DEBUG\n",
    "    plt.imshow(gray_image)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    \n",
    "    # another process ...\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "148d05cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KODE INI KAMI GUNAKAN UNTUK MENYAMAKAN RATIO (DALAM KONTEKS GAMBAR YANG DIINPUT SAAT MENJALANKAN MODEL JIKA RATIO GAMBAR TIDAK SAMA SEPERTI DI TRAINING)\n",
    "\n",
    "aspect_ratio = 16 / 9\n",
    "\n",
    "def calculate_resolution(width, aspect_ratio):\n",
    "    new_width = width\n",
    "    new_height = int(width / aspect_ratio)\n",
    "    return (new_width, new_height)\n",
    "\n",
    "RESHAPE = calculate_resolution(256, aspect_ratio)\n",
    "\n",
    "def preprocess_image(cv_img):\n",
    "    # Resize image\n",
    "    cv_img = cv2.resize(cv_img, RESHAPE)\n",
    "    \n",
    "    # Convert color space to Lab\n",
    "    lab_img = cv2.cvtColor(cv_img, cv2.COLOR_BGR2LAB)\n",
    "    \n",
    "    # Flatten image for KMeans clustering\n",
    "    flattened_img = lab_img.reshape((-1, 3))\n",
    "    \n",
    "    # Perform color quantization using KMeans clustering\n",
    "    kmeans = KMeans(n_clusters=8)\n",
    "    kmeans.fit(flattened_img)\n",
    "    quantized_colors = kmeans.cluster_centers_.astype(np.uint8)\n",
    "    labels = kmeans.labels_\n",
    "    quantized_img = quantized_colors[labels].reshape(RESHAPE[1], RESHAPE[0], 3)\n",
    "    \n",
    "    # Normalize pixel values to the range [-1, 1]\n",
    "    normalized_img = (quantized_img.astype(float32) - 127.5) / 127.5\n",
    "    \n",
    "    return normalized_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9a8967f-18f8-41b9-abf8-7d3d3b742011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ulos_type</th>\n",
       "      <th>path_color</th>\n",
       "      <th>path_gray</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>harungguan</td>\n",
       "      <td>Dataset\\harungguan/Harungguan Biru_Ruth Theres...</td>\n",
       "      <td>Output Grayscale\\harungguan/Harungguan Biru_Ru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>harungguan</td>\n",
       "      <td>Dataset\\harungguan/Harungguan Coklat Terang_Ru...</td>\n",
       "      <td>Output Grayscale\\harungguan/Harungguan Coklat ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>harungguan</td>\n",
       "      <td>Dataset\\harungguan/Harungguan Hijau_Ruth There...</td>\n",
       "      <td>Output Grayscale\\harungguan/Harungguan Hijau_R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>harungguan</td>\n",
       "      <td>Dataset\\harungguan/Harungguan Merah_Ruth There...</td>\n",
       "      <td>Output Grayscale\\harungguan/Harungguan Merah_R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>harungguan</td>\n",
       "      <td>Dataset\\harungguan/Harungguan Orange_Ruth Ther...</td>\n",
       "      <td>Output Grayscale\\harungguan/Harungguan Orange_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>sadum</td>\n",
       "      <td>Dataset\\sadum/sadum_sri-rahayu_010.png</td>\n",
       "      <td>Output Grayscale\\sadum/sadum_sri-rahayu_010.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>sadum</td>\n",
       "      <td>Dataset\\sadum/Sadum_Theresia-Yolanda_001.png</td>\n",
       "      <td>Output Grayscale\\sadum/Sadum_Theresia-Yolanda_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>sadum</td>\n",
       "      <td>Dataset\\sadum/Sadum_Theresia-Yolanda_002.png</td>\n",
       "      <td>Output Grayscale\\sadum/Sadum_Theresia-Yolanda_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>sadum</td>\n",
       "      <td>Dataset\\sadum/Sadum_Theresia-Yolanda_003.png</td>\n",
       "      <td>Output Grayscale\\sadum/Sadum_Theresia-Yolanda_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>sadum</td>\n",
       "      <td>Dataset\\sadum/Sadum_Theresia-Yolanda_004.png</td>\n",
       "      <td>Output Grayscale\\sadum/Sadum_Theresia-Yolanda_...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ulos_type                                         path_color  \\\n",
       "0   harungguan  Dataset\\harungguan/Harungguan Biru_Ruth Theres...   \n",
       "1   harungguan  Dataset\\harungguan/Harungguan Coklat Terang_Ru...   \n",
       "2   harungguan  Dataset\\harungguan/Harungguan Hijau_Ruth There...   \n",
       "3   harungguan  Dataset\\harungguan/Harungguan Merah_Ruth There...   \n",
       "4   harungguan  Dataset\\harungguan/Harungguan Orange_Ruth Ther...   \n",
       "..         ...                                                ...   \n",
       "59       sadum             Dataset\\sadum/sadum_sri-rahayu_010.png   \n",
       "60       sadum       Dataset\\sadum/Sadum_Theresia-Yolanda_001.png   \n",
       "61       sadum       Dataset\\sadum/Sadum_Theresia-Yolanda_002.png   \n",
       "62       sadum       Dataset\\sadum/Sadum_Theresia-Yolanda_003.png   \n",
       "63       sadum       Dataset\\sadum/Sadum_Theresia-Yolanda_004.png   \n",
       "\n",
       "                                            path_gray  \n",
       "0   Output Grayscale\\harungguan/Harungguan Biru_Ru...  \n",
       "1   Output Grayscale\\harungguan/Harungguan Coklat ...  \n",
       "2   Output Grayscale\\harungguan/Harungguan Hijau_R...  \n",
       "3   Output Grayscale\\harungguan/Harungguan Merah_R...  \n",
       "4   Output Grayscale\\harungguan/Harungguan Orange_...  \n",
       "..                                                ...  \n",
       "59    Output Grayscale\\sadum/sadum_sri-rahayu_010.png  \n",
       "60  Output Grayscale\\sadum/Sadum_Theresia-Yolanda_...  \n",
       "61  Output Grayscale\\sadum/Sadum_Theresia-Yolanda_...  \n",
       "62  Output Grayscale\\sadum/Sadum_Theresia-Yolanda_...  \n",
       "63  Output Grayscale\\sadum/Sadum_Theresia-Yolanda_...  \n",
       "\n",
       "[64 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50e33be6-85c2-4523-93ce-ea83e8a1de8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KODE HASIL GRAYSCALE DIGABUNG DENGAN GAMBAR ASLI\n",
    "\n",
    "def convert_to_grayscale(input_folder, output_folder):\n",
    "    # Membuat folder output jika belum ada\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    # Membuat daftar sub-folder di dalam folder input\n",
    "    sub_folders = os.listdir(input_folder)\n",
    "    \n",
    "    # Iterasi melalui setiap sub-folder\n",
    "    for sub_folder in sub_folders:\n",
    "        # Membuat path lengkap ke sub-folder\n",
    "        sub_folder_path = os.path.join(input_folder, sub_folder)\n",
    "        \n",
    "        # Memeriksa apakah itu adalah direktori\n",
    "        if not os.path.isdir(sub_folder_path):\n",
    "            print(f\"{sub_folder_path} bukan direktori. Dilewati.\")\n",
    "            continue\n",
    "        \n",
    "        # Membuat folder output untuk setiap kategori\n",
    "        output_sub_folder = os.path.join(output_folder, sub_folder)\n",
    "        if not os.path.exists(output_sub_folder):\n",
    "            os.makedirs(output_sub_folder)\n",
    "        \n",
    "        # Membuat daftar file dalam sub-folder\n",
    "        image_files = os.listdir(sub_folder_path)\n",
    "        \n",
    "        # Iterasi melalui setiap file gambar dalam sub-folder\n",
    "        for image_file in image_files:\n",
    "            img_path = os.path.join(sub_folder_path, image_file)\n",
    "            \n",
    "            # Membaca gambar\n",
    "            img = cv2.imread(img_path)\n",
    "            \n",
    "            # Cek apakah gambar berhasil dibaca\n",
    "            if img is None:\n",
    "                print(f\"Gagal membaca gambar: {image_file}\")\n",
    "                continue\n",
    "            \n",
    "            # Salin gambar asli ke dalam folder output\n",
    "            output_original_path = os.path.join(output_sub_folder, image_file)\n",
    "            shutil.copy(img_path, output_original_path)\n",
    "            \n",
    "            # Konversi ke grayscale\n",
    "            gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            \n",
    "            # Menyusun path lengkap dari output file untuk gambar grayscale\n",
    "            output_gray_path = os.path.join(output_sub_folder, f\"gray_{image_file}\")\n",
    "            \n",
    "            # Menyimpan gambar grayscale\n",
    "            cv2.imwrite(output_gray_path, gray_img)\n",
    "\n",
    "# Direktori dataset\n",
    "dataset_dir = \"Dataset\"\n",
    "\n",
    "# Direktori untuk menyimpan gambar grayscale\n",
    "output_folder = \"Dataset_with_Grayscale\"\n",
    "\n",
    "# Menyusun path lengkap dari output folder\n",
    "output_path = output_folder\n",
    "\n",
    "convert_to_grayscale(dataset_dir, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ff75822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PERBAIKAN KODE JIKA SESUAI DENGAN PENGUBAHAN RATIO\n",
    "\n",
    "def preprocess_batch_from_aug(imgs):\n",
    "    final_imgs = np.zeros_like(imgs)\n",
    "    \n",
    "    for i in range(len(imgs)):\n",
    "        final_img = preprocess_image(imgs[i]) \n",
    "        final_imgs[i] = final_img \n",
    "        \n",
    "    l_imgs = final_imgs[:,:,:,0]\n",
    "    l_imgs = np.expand_dims(l_imgs, axis=3)\n",
    "    ab_imgs = final_imgs[:,:,:,1:]\n",
    "    return l_imgs, ab_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b8708f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESHAPE = (int(1440/4), int(3600/4))\n",
    "RESHAPE = (256, 256)\n",
    "\n",
    "def preprocess_image(cv_img):\n",
    "    cv_img = cv2.resize(cv_img, RESHAPE)\n",
    "    img = (cv_img - 127.5) / 127.5\n",
    "    return img\n",
    "\n",
    "def deprocess_image(img):\n",
    "    img = (img * 127.5) + 127.5\n",
    "    return img.astype('uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3178a598",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_batch_from_aug(imgs):\n",
    "    final_imgs = np.zeros((imgs.shape))\n",
    "    \n",
    "    imgs = imgs.astype(np.uint8)\n",
    "    \n",
    "    for i in range(len(imgs)):\n",
    "        final_img = cv2.cvtColor(imgs[i], cv2.COLOR_RGB2LAB)\n",
    "        final_img = preprocess_image(final_img) \n",
    "        final_imgs[i] = final_img \n",
    "        \n",
    "    l_imgs = final_imgs[:,:,:,0]\n",
    "    l_imgs = np.expand_dims(l_imgs, axis=3)\n",
    "    ab_imgs = final_imgs[:,:,:,1:]\n",
    "    return l_imgs, ab_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "18a69b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_an_image_file(filename):\n",
    "    IMAGE_EXTENSIONS = ['.png', '.jpg', '.jpeg']\n",
    "    for ext in IMAGE_EXTENSIONS:\n",
    "        if ext in filename:\n",
    "            return True\n",
    "    return False\n",
    "    \n",
    "def list_image_files(base_path):\n",
    "    directory_generator = os.walk(base_path)\n",
    "    next(directory_generator)\n",
    "    path_tree = {}\n",
    "    for root_path, directories, files in directory_generator:\n",
    "        path_tree[os.path.basename(root_path)] = [\n",
    "            os.path.join(root_path, file_path) for file_path in files]\n",
    "    return sum(list(path_tree.values()), [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e35db953",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(path):\n",
    "    img = cv2.imread(path[0])\n",
    "    \n",
    "    # Make sure all images are 256 x 256 by cropping them\n",
    "    r, c = img.shape[:2]\n",
    "    r_diff = (r - 256) // 2\n",
    "    c_diff = (c - 256) // 2\n",
    "    cropped = img[r_diff:256 + r_diff, c_diff:256 + c_diff]\n",
    "    return cropped\n",
    "    # return img\n",
    "\n",
    "def load_images(path, n_images=-1):\n",
    "    all_image_paths = list_image_files(path)\n",
    "    \n",
    "    if n_images < 0:\n",
    "        n_images = len(all_image_paths)\n",
    "    images_l, images_ab = [], []\n",
    "    \n",
    "    # Initialize a progress bar with max of n_images\n",
    "    pbar = tqdm(total = n_images, desc = \"Loading Images...\")\n",
    "    \n",
    "    for path in zip(all_image_paths):\n",
    "        img = load_image(path)\n",
    "        lab_img = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
    "        lab_img = preprocess_image(lab_img)\n",
    "        \n",
    "        l = lab_img[:,:,0]\n",
    "        l = l[:,:,np.newaxis]\n",
    "        # Include all 3 channels, overwrite 1st channel with 0's\n",
    "        ab = lab_img[:,:,1:]\n",
    "\n",
    "        images_l.append(l)\n",
    "        images_ab.append(ab)\n",
    "\n",
    "        images_loaded = len(images_l)\n",
    "        \n",
    "        # Increase progress by one\n",
    "        pbar.update(1)\n",
    "        \n",
    "        if images_loaded > n_images - 1: \n",
    "            break\n",
    "\n",
    "    return {\n",
    "        'l': np.array(images_l),\n",
    "        'ab': np.array(images_ab)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bcb80469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_images(df):\n",
    "#     df_res = df.copy()\n",
    "#     list_path = list(df.path)\n",
    "\n",
    "# def load_images(directory):\n",
    "#     df_res = df.copy()\n",
    "#     list_path = list(df.path)\n",
    "    \n",
    "#     images_l = []\n",
    "#     images_ab = []\n",
    "#     for path in tqdm(list_path):\n",
    "#         img = cv2.imread(path)\n",
    "#         img = cv2.resize(img, (360, 900))\n",
    "#         lab_img = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
    "\n",
    "#         l = lab_img[:,:,0]\n",
    "#         l = l[:,:,np.newaxis]\n",
    "#         # Include all 3 channels, overwrite 1st channel with 0's\n",
    "#         ab = lab_img[:,:,1:]\n",
    "\n",
    "#         images_l.append(l)\n",
    "#         images_ab.append(ab)\n",
    "\n",
    "#     df_res['l'] = images_l\n",
    "#     df_res['ab'] = images_ab\n",
    "    \n",
    "#     return df_res\n",
    "\n",
    "# DEBUG\n",
    "# load_images(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "09de17bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "753da663a143408799be066b4769e5ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading Images...:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = load_images(\"Dataset\")\n",
    "# display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3eac09f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_imgs_new(truth, gray, predictions, n_test_batch, shouldSave, sample_title=None, show_GT=True):    \n",
    "    # Store all merged\n",
    "    merged_predictions = []\n",
    "    merged_grays = []\n",
    "    merged_truths = []\n",
    "        \n",
    "    # For each image...\n",
    "    for i in range(n_test_batch):\n",
    "        # Get prediction, merge\n",
    "        merged = []\n",
    "        # For each k...\n",
    "        for j in range(k):\n",
    "            # Generate a prediction...\n",
    "            prediction = predictions[i,:,:,2*j:(2*j)+2].astype(float64)\n",
    "            # And merge it to be a 3 channel image\n",
    "            merged.append(cv2.merge((gray[i], prediction)))\n",
    "        merged_predictions.append(merged) \n",
    "        \n",
    "        # Merge the grayscale channels\n",
    "        merged_grays.append(cv2.merge((gray[i], gray[i], gray[i])))\n",
    "        \n",
    "        # Merge the l and ab to create truth\n",
    "        merged_truths.append(cv2.merge((gray[i], truth[i])))\n",
    "\n",
    "    \n",
    "    r = truth.shape[1]\n",
    "    c = truth.shape[2]\n",
    "    \n",
    "    start_r = 0\n",
    "\n",
    "    n_predictions = k\n",
    "    # Create figure\n",
    "    columns = 1 + n_predictions\n",
    "    if show_GT:\n",
    "        columns += 1\n",
    "    figure = np.zeros([r * n_test_batch + (15 * (n_test_batch-1)), (c * columns) + 5 * (columns-1), 3], dtype=np.uint8)\n",
    "    figure += 255\n",
    "    \n",
    "    # Loop through sets of test images\n",
    "    for i in range(n_test_batch):\n",
    "        # Place truth and gray\n",
    "        if show_GT:\n",
    "            figure[start_r:start_r + r, :c] = cv2.cvtColor(deprocess_image(merged_truths[i]), cv2.COLOR_LAB2RGB)\n",
    "            figure[start_r:start_r + r, c + 5:2*c + 5] = deprocess_image(merged_grays[i])\n",
    "        else:\n",
    "            figure[start_r:start_r + r, :c] = deprocess_image(merged_grays[i])\n",
    "\n",
    "\n",
    "\n",
    "        # Place multiple preditions in figure\n",
    "        for j in range(n_predictions):  \n",
    "            current_predictions = merged_predictions[i]\n",
    "            if show_GT:\n",
    "                figure[start_r:start_r + r, (2+j)*(c+5):(3+j)*c + (5*(2+j))] = \\\n",
    "                    cv2.cvtColor(deprocess_image(current_predictions[j]), cv2.COLOR_LAB2RGB)\n",
    "            else:\n",
    "                figure[start_r:start_r + r, (1+j)*(c+5):(2+j)*c + (5*(1+j))] = \\\n",
    "                    cv2.cvtColor(deprocess_image(current_predictions[j]), cv2.COLOR_LAB2RGB)\n",
    "        start_r += r + 15\n",
    "    img = Image.fromarray(figure, \"RGB\")\n",
    "\n",
    "    dpi = plt.rcParams['figure.dpi']\n",
    "    height, width, _ = figure.shape\n",
    "\n",
    "    preview = plt.figure(figsize=(6,4))\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "    title = sample_title\n",
    "    plt.title(title, fontsize = 'large')\n",
    "    plt.show()\n",
    "\n",
    "    if shouldSave:\n",
    "        figsize = width / float(dpi), height / float(dpi)\n",
    "\n",
    "        full = plt.figure(figsize = figsize)\n",
    "        plt.imshow(img)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(title, fontsize = 25)\n",
    "        plt.savefig(save_path + sample_title + \".png\")\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0d0e27",
   "metadata": {},
   "source": [
    "### 4. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ce3aa1",
   "metadata": {},
   "source": [
    "#### 4.1 Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "572f71fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_generator(H, W, k):\n",
    "    # Inputs: height and width of the input image\n",
    "    # Returns the model, which generates the AB channels\n",
    "\n",
    "    # Pix2pix adapted from \n",
    "    # https://github.com/eriklindernoren/Keras-GAN/blob/master/pix2pix/pix2pix.py\n",
    "\n",
    "    def conv2d(layer_input, filters, f_size=4, bn=True):\n",
    "        \"\"\"Layers used during downsampling\"\"\"\n",
    "        d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
    "        d = LeakyReLU(alpha=0.2)(d)\n",
    "        if bn:\n",
    "            d = BatchNormalization(momentum=0.8)(d)\n",
    "        return d\n",
    "\n",
    "    def deconv2d(layer_input, skip_input, filters, f_size=4, dropout_rate=0):\n",
    "        \"\"\"Layers used during upsampling\"\"\"\n",
    "        u = UpSampling2D(size=2)(layer_input)\n",
    "        u = Conv2D(filters, kernel_size=f_size, strides=1, padding='same', activation='relu')(u)\n",
    "        if dropout_rate:\n",
    "            u = Dropout(dropout_rate)(u)\n",
    "        u = BatchNormalization(momentum=0.8)(u)\n",
    "        u = Concatenate()([u, skip_input])\n",
    "        return u\n",
    "\n",
    "    gf = 64 # Number of filters in the first layer of G\n",
    "\n",
    "    noise_in = Input(shape=(100,))\n",
    "    condition_in = Input(shape=(H, W, 1))\n",
    "    \n",
    "    # pass noise through a FC layer to get it to the right size\n",
    "    noise = Dense(H * H)(noise_in)\n",
    "\n",
    "    # reshape to be the size of an image channel\n",
    "    noise = Reshape((H, H, 1))(noise)\n",
    "    \n",
    "    # stick the (somewhat modified) noise as the second channel after\n",
    "    # the gray input. Assuming new dimension of hid will be\n",
    "    # B x 256 x 256 x 2, where B is the batch size.\n",
    "    if use_noise:\n",
    "        d0 = Concatenate(axis=-1)([condition_in, noise])\n",
    "        print(\"* * *\")\n",
    "        print(\"USING NOISE\")\n",
    "        print(\"* * *\")\n",
    "    else:\n",
    "        d0 = condition_in \n",
    "        print(\"* * *\")\n",
    "        print(\"NO NOISE\")\n",
    "        print(\"* * *\")\n",
    "        \n",
    "    # U-NET\n",
    "    # Downsampling\n",
    "    d1 = conv2d(d0, gf, bn=False)\n",
    "    d2 = conv2d(d1, gf*2)\n",
    "    d3 = conv2d(d2, gf*4)\n",
    "    d4 = conv2d(d3, gf*8)\n",
    "    d5 = conv2d(d4, gf*8)\n",
    "    d6 = conv2d(d5, gf*8)\n",
    "    d7 = conv2d(d6, gf*8)\n",
    "\n",
    "    # Upsampling\n",
    "    u1 = deconv2d(d7, d6, gf*8)\n",
    "    u2 = deconv2d(u1, d5, gf*8)\n",
    "    u3 = deconv2d(u2, d4, gf*8)\n",
    "    u4 = deconv2d(u3, d3, gf*4)\n",
    "    u5 = deconv2d(u4, d2, gf*2)\n",
    "    u6 = deconv2d(u5, d1, gf)\n",
    "\n",
    "    u7 = UpSampling2D(size=2)(u6)\n",
    "    \n",
    "    # Final 2-channel AB image with values between -1 and 1\n",
    "    img_out = Conv2D(2*k, kernel_size=4, strides=1, padding='same', activation='tanh', name='pred_ab')(u7)\n",
    "\n",
    "    # Make Model\n",
    "    model = Model(inputs=[noise_in, condition_in], outputs=img_out)\n",
    "    \n",
    "    # Show summary of layers\n",
    "    print(\"Generator Model:\")\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9cf096",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_discriminator(H, W, k):\n",
    "    # Inputs: height and width of the input image\n",
    "    # Returns the model, which predicts real/fake\n",
    "    # over a set of spatial regions (i.e., predicts a matrix instead of a scalar).\n",
    "\n",
    "    # Pix2pix adapted from \n",
    "    # https://github.com/eriklindernoren/Keras-GAN/blob/master/pix2pix/pix2pix.py\n",
    "\n",
    "    def d_layer(layer_input, filters, f_size=4, bn=True):\n",
    "        \"\"\"Discriminator layer\"\"\"\n",
    "        d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
    "        d = LeakyReLU(alpha=0.2)(d)\n",
    "        if bn:\n",
    "            d = BatchNormalization(momentum=0.8)(d)\n",
    "        return d\n",
    "\n",
    "    # Number of filters in the first layer of D\n",
    "    df = 64\n",
    "\n",
    "    img_in = Input(shape=(H, W, 2*k)) # AB channels\n",
    "    condition_in = Input(shape=(H, W, 1)) # L channel\n",
    "    \n",
    "    # Concat the L and AB channels\n",
    "    concat_imgs = Concatenate()([condition_in, img_in])\n",
    "\n",
    "    d1 = d_layer(concat_imgs, df, bn=False)\n",
    "    d2 = d_layer(d1, df*2)\n",
    "    d3 = d_layer(d2, df*4)\n",
    "    d4 = d_layer(d3, df*8)\n",
    "\n",
    "    # validity map is a one-channel matrix 1/16 the size of the input (halved 4 times).\n",
    "    # Each number predicts whether a region of the input is real/fake.\n",
    "    validity = Conv2D(1*k, kernel_size=4, strides=1, padding='same', name='pred_valid')(d4)\n",
    "\n",
    "    # Build Model\n",
    "    model = Model(inputs=[img_in, condition_in], outputs=validity)\n",
    "\n",
    "    # Show summary of layers\n",
    "    print(\"Disciminator Model:\")\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f13931",
   "metadata": {},
   "outputs": [],
   "source": [
    "def histogram(tensor, nbins=10, axis=None):\n",
    "#     value_range = [tf.reduce_min(tensor), tf.reduce_max(tensor)]\n",
    "\n",
    "    value_range = [-1, 1]\n",
    "\n",
    "    if axis is None:\n",
    "        return tf.histogram_fixed_width(tensor, value_range, nbins=nbins)\n",
    "    else:\n",
    "        if not hasattr(axis, \"__len__\"):\n",
    "            axis = [axis]\n",
    "\n",
    "        other_axis = [x for x in range(0, len(tensor.shape)) if x not in axis]\n",
    "        swap = tf.transpose(tensor, [*other_axis, *axis])\n",
    "        flat = tf.reshape(swap, [-1, *np.take(tensor.shape.as_list(), axis)])\n",
    "\n",
    "        count = tf.map_fn(lambda x: tf.histogram_fixed_width(x, value_range, nbins=nbins), flat, tf.int32)\n",
    "\n",
    "        return tf.reshape(count, [*np.take([-1 if a is None else a for a in tensor.shape.as_list()], other_axis), nbins])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d43a1fa",
   "metadata": {},
   "source": [
    "#### 4.2. Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2dd115",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_k_diff(y_true, y_pred):\n",
    "    # Shape: (Batch, H, W, k, 2)\n",
    "    y_true = tf.reshape(y_true, (-1, H, W, k, 2))\n",
    "    y_pred = tf.reshape(y_pred, (-1, H, W, k, 2))\n",
    "\n",
    "    print(\"true:\", y_true.shape)\n",
    "    print(\"pred:\", y_pred.shape)\n",
    "\n",
    "    diff = y_true - y_pred\n",
    "    diff = tf.math.abs(diff)\n",
    "    diff = tf.math.reduce_mean(diff, axis=(1, 2, 4)) # mean of (H, W, 2) leaves (B, k)\n",
    "    \n",
    "    loss_metric = diff\n",
    "\n",
    "    min_for_each_batch = tf.math.reduce_min(loss_metric, axis=1)\n",
    "    return tf.math.reduce_sum(min_for_each_batch) #* .01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "625a553e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_k_diff_plus_k_entropy_diff(y_true, y_pred):\n",
    "    # Shape: (Batch, H, W, k, 2)\n",
    "    y_true = tf.reshape(y_true, (-1, H, W, k, 2))\n",
    "    y_pred = tf.reshape(y_pred, (-1, H, W, k, 2))\n",
    "\n",
    "    diff = y_true - y_pred\n",
    "    diff = tf.math.abs(diff)\n",
    "    diff = tf.math.reduce_mean(diff, axis=(1, 2, 4)) # mean of (H, W, 2) leaves (B, k)\n",
    "    \n",
    "    true_hist_ab = histogram(y_true, nbins=25, axis=[1,2]) # hist of (H, W) leaves (B, k, 2, bins)\n",
    "    \n",
    "    pred_hist_ab = histogram(y_pred, nbins=25, axis=[1,2]) # hist of (H, W) leaves (B, k, 2, bins)\n",
    "    \n",
    "    true_hist_ab /= H * W\n",
    "    pred_hist_ab /= H * W\n",
    "    \n",
    "    true_hist_ab = K.clip(true_hist_ab, K.epsilon(), 1-K.epsilon())\n",
    "    pred_hist_ab = K.clip(pred_hist_ab, K.epsilon(), 1-K.epsilon())\n",
    "\n",
    "    # Sum of -plogp (entropy) across bins\n",
    "    true_hist_sum = -tf.math.reduce_sum(true_hist_ab * K.log(true_hist_ab), axis=[3]) # sum of (bins) leaves (B, k, 2)\n",
    "    pred_hist_sum = -tf.math.reduce_sum(pred_hist_ab * K.log(pred_hist_ab), axis=[3]) # sum of (bins) leaves (B, k, 2)\n",
    "    \n",
    "    # Take the difference of the entropies\n",
    "    diff_entropies = tf.math.abs(true_hist_sum - pred_hist_sum)\n",
    "    \n",
    "    # Calc mean abs difference for each k\n",
    "    mad_entropies = tf.math.reduce_mean(diff_entropies, axis=2) # mean of (B, k, 2) leaves (B, k)\n",
    "    \n",
    "    loss_metric = diff\n",
    "    best_k = K.argmin(loss_metric, axis=1)\n",
    "    \n",
    "    # Get indexes for best k\n",
    "    batch_range = K.arange(BATCH_SIZE, dtype='int64')\n",
    "    batch_range = tf.reshape(batch_range, (BATCH_SIZE, 1))\n",
    "    best_k = tf.reshape(best_k, (BATCH_SIZE, 1))\n",
    "    idx = K.concatenate((batch_range, best_k), axis=1)\n",
    "    \n",
    "    # Slice to get mins\n",
    "    min_for_each_n = tf.gather_nd(loss_metric, idx)\n",
    "    diff_entropy_each_n = tf.gather_nd(mad_entropies, idx)\n",
    "\n",
    "    return (tf.math.reduce_sum(min_for_each_n) + (.01 * tf.math.reduce_sum(K.cast(diff_entropy_each_n, dtype=\"float32\")))) #/ 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2235e1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_k_chroma(y_true, y_pred):\n",
    "    # Shape: (Batch, H, W, k, 2)\n",
    "    y_true = tf.reshape(y_true, (-1, H, W, k, 2))\n",
    "    y_pred = tf.reshape(y_pred, (-1, H, W, k, 2))\n",
    "\n",
    "    truth_chroma = tf.math.reduce_sum(K.square(y_true), axis=(4)) # sum of (2) squared leaves (B, H, W, k)\n",
    "    pred_chroma = tf.math.reduce_sum(K.square(y_pred), axis=(4)) # sum of (2) squared leaves (B, H, W, k)\n",
    "    \n",
    "    sqrt_truth_chroma = K.sqrt(truth_chroma)\n",
    "    sqrt_pred_chroma = K.sqrt(pred_chroma)\n",
    "\n",
    "    diff_chroma = tf.math.abs(truth_chroma - pred_chroma) # still (B, H, W, k)\n",
    "    \n",
    "    sum_diff_chroma = tf.math.reduce_sum(diff_chroma, axis=(1, 2)) # Leave (B, k)\n",
    "    \n",
    "    # Add Sum of Average Differences (SAD) term\n",
    "    diff = y_true - y_pred\n",
    "    diff = tf.math.abs(diff)\n",
    "    diff = tf.math.reduce_mean(diff, axis=(1, 2, 4)) # mean of (H, W, 2) leaves (B, k)\n",
    "    \n",
    "    transf_sum_diff_chroma = sum_diff_chroma / 1e7 # Values between 0-10\n",
    "    transf_diff = diff / 35 / 10 # Values between 0-1\n",
    "    \n",
    "    \n",
    "    loss_metric = transf_sum_diff_chroma + transf_diff # (B, k)\n",
    "    \n",
    "    min_for_each_batch = tf.math.reduce_min(loss_metric, axis=1) # Choose best k for each B\n",
    "    return tf.math.reduce_sum(min_for_each_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e803ecba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_dist_between_chroma_preds(y_pred):\n",
    "    distances = []\n",
    "    \n",
    "    count = 0\n",
    "    for i in range(k):\n",
    "        for j in range(i + 1, k):\n",
    "            diff = tf.math.reduce_sum(K.square(y_pred[:, :, :, i] - y_pred[:, :, :, j]), axis=(1,2)) # (B)\n",
    "            distances.append(diff)\n",
    "            count += 1\n",
    "    stacked = K.stack(distances, axis=1) \n",
    "    print(\"Stacked:\", stacked.shape)\n",
    "    return -tf.math.reduce_min(stacked, axis=1) # We want to maximize the distance between predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b558123",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_dist_between_color_preds(y_pred):\n",
    "    # y_pred is (B, H, W, k, 2)\n",
    "    distances = []\n",
    "    \n",
    "    count = 0\n",
    "    for i in range(k):\n",
    "        for j in range(i + 1, k):\n",
    "            diff = y_pred[:, :, :, i, :] - y_pred[:, :, :, j, :] # (B, H, W, 2)\n",
    "            sum_diff = tf.math.reduce_sum(K.square(diff), axis=(1,2,3)) # (B)\n",
    "            distances.append(sum_diff)\n",
    "            count += 1\n",
    "    stacked = K.stack(distances, axis=1)\n",
    "    print(\"Stacked:\", stacked.shape)\n",
    "    return -tf.math.reduce_min(stacked, axis=1) # We want to maximize the distance between predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a2bfc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_dist_between_hue_angles(y_pred):\n",
    "    # y_pred is (B, H, W, k, 2)\n",
    "    distances = []\n",
    "    \n",
    "    for i in range(k):\n",
    "        for j in range(i + 1, k):\n",
    "            # https://stackoverflow.com/a/2007279\n",
    "            angle_i = tf.math.atan2(y_pred[:, :, :, i, 0], y_pred[:, :, :, i, 1])\n",
    "            angle_j = tf.math.atan2(y_pred[:, :, :, j, 0], y_pred[:, :, :, j, 1])\n",
    "            diff = tf.math.atan2(tf.math.sin(angle_i - angle_j), tf.math.cos(angle_i - angle_j)) # (B, H, W)\n",
    "            sum_diff = tf.math.reduce_sum(K.square(diff), axis=(1,2)) # (B)\n",
    "            distances.append(sum_diff)\n",
    "    stacked = K.stack(distances, axis=1)\n",
    "    print(\"Stacked:\", stacked.shape)\n",
    "    return -tf.math.reduce_min(stacked, axis=1) # We want to maximize the distance between predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58a097e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_k_chroma_with_pairwise_difference_k(y_true, y_pred):\n",
    "    # Shape: (Batch, H, W, k, 2)\n",
    "    y_true = tf.reshape(y_true, (-1, H, W, k, 2))\n",
    "    y_pred = tf.reshape(y_pred, (-1, H, W, k, 2))\n",
    "\n",
    "    truth_chroma = tf.math.reduce_sum(K.square(y_true), axis=(4)) # sum of (2) squared leaves (B, H, W, k)\n",
    "    pred_chroma = tf.math.reduce_sum(K.square(y_pred), axis=(4)) # sum of (2) squared leaves (B, H, W, k)\n",
    "    \n",
    "    sqrt_truth_chroma = K.sqrt(truth_chroma)\n",
    "    sqrt_pred_chroma = K.sqrt(pred_chroma)\n",
    "    \n",
    "    diff_chroma = tf.math.abs(sqrt_truth_chroma - sqrt_pred_chroma) # still (B, H, W, k)\n",
    "    \n",
    "    sum_diff_chroma = tf.math.reduce_sum(diff_chroma, axis=(1, 2)) # Leave (B, k)\n",
    "\n",
    "    transf_sum_diff_chroma = sum_diff_chroma / 1e7 # Values between 0-10\n",
    "    \n",
    "    min_dist_hue = min_dist_between_hue_angles(y_pred) / 1e7 / 2 / 2 # Values between 0 and -10\n",
    "\n",
    "    return tf.math.reduce_sum(transf_sum_diff_chroma) + tf.math.reduce_sum(min_dist_hue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8f8125f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_k_diff_minus_k_std(y_true, y_pred):\n",
    "    # Shape: (Batch, H, W, k, 2)\n",
    "    y_true = tf.reshape(y_true, (-1, H, W, k, 2))\n",
    "    y_pred = tf.reshape(y_pred, (-1, H, W, k, 2))\n",
    "\n",
    "    print(\"true:\", y_true.shape)\n",
    "    print(\"pred:\", y_pred.shape)\n",
    "\n",
    "    diff = y_true - y_pred\n",
    "    diff = tf.math.abs(diff)\n",
    "    diff = tf.math.reduce_mean(diff, axis=(1, 2, 4))\n",
    "    std_dev = K.std(y_pred, axis=(1, 2, 4))\n",
    "    \n",
    "    loss_metric = diff - (0.05 * std_dev)\n",
    "    min_for_each_batch = tf.math.reduce_min(loss_metric, axis=1)\n",
    "    return tf.math.reduce_sum(min_for_each_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad570fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_noise(n_samples, noise_dim):\n",
    "    X = np.random.normal(0, 1, size=(n_samples, noise_dim))\n",
    "    return X\n",
    "\n",
    "def show_samples(batchidx, save_path):\n",
    "    fig, axs = plt.subplots(5, 6, figsize=(10,6))\n",
    "    plt.subplots_adjust(hspace=0.3, wspace=0.1)\n",
    "    #fig, axs = plt.subplots(5, 6)\n",
    "    #fig.tight_layout()\n",
    "    for classlabel in range(10):\n",
    "        row = int(classlabel / 2)\n",
    "        coloffset = (classlabel % 2) * 3\n",
    "        lbls = one_hot_encode([classlabel] * 3)\n",
    "        noise = generate_noise(3, 100)\n",
    "        gen_imgs = generator.predict([noise, lbls])\n",
    "\n",
    "        for i in range(3):\n",
    "            # Dont scale the images back, let keras handle it\n",
    "            img = image.array_to_img(gen_imgs[i], scale=True)\n",
    "            axs[row,i+coloffset].imshow(img)\n",
    "            axs[row,i+coloffset].axis('off')\n",
    "            if i ==1:\n",
    "                axs[row,i+coloffset].set_title(tags[classlabel])\n",
    "    plt.savefig(save_path + '.jpg')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "605cec35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(g_losses, d_losses, epoch_num, scale_factor):\n",
    "    # Scale axis so it lines up with epoch_num\n",
    "    x_axis = [epoch_num * x / (len(g_losses) - 1) for x in range(len(g_losses))]\n",
    "    \n",
    "    g_scaled = [x * scale_factor for x in g_losses]\n",
    "    \n",
    "    ax = subplot(1,1,1)\n",
    "    ax.plot(x_axis, g_scaled, label=\"Generator\")\n",
    "    ax.plot(x_axis, d_losses, label=\"Discriminator\")\n",
    "    plt.title(\"Losses After \" + str(epoch_num) + \" Epochs\")\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    ax.legend(handles, labels)\n",
    "    plt.savefig(save_path + \"Losses.jpg\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a7369b",
   "metadata": {},
   "source": [
    "#### 4.3. Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea39f17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_k_diff(y_true, y_pred):\n",
    "    # Shape: (Batch, H, W, k, 2)\n",
    "    y_true = tf.reshape(y_true, (-1, H, W, k, 2))\n",
    "    y_pred = tf.reshape(y_pred, (-1, H, W, k, 2))\n",
    "\n",
    "    print(\"true:\", y_true.shape)\n",
    "    print(\"pred:\", y_pred.shape)\n",
    "\n",
    "    diff = y_true - y_pred\n",
    "    diff = tf.math.abs(diff)\n",
    "    diff = tf.math.reduce_mean(diff, axis=(1, 2, 4)) # mean of (H, W, 2) leaves (B, k)\n",
    "    \n",
    "    loss_metric = diff\n",
    "\n",
    "    min_for_each_batch = tf.math.reduce_min(loss_metric, axis=1)\n",
    "    return tf.math.reduce_sum(min_for_each_batch) #* .01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "19f4abc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_k_diff_plus_k_entropy_diff(y_true, y_pred):\n",
    "    # Shape: (Batch, H, W, k, 2)\n",
    "    y_true = tf.reshape(y_true, (-1, H, W, k, 2))\n",
    "    y_pred = tf.reshape(y_pred, (-1, H, W, k, 2))\n",
    "\n",
    "    diff = y_true - y_pred\n",
    "    diff = tf.math.abs(diff)\n",
    "    diff = tf.math.reduce_mean(diff, axis=(1, 2, 4)) # mean of (H, W, 2) leaves (B, k)\n",
    "    \n",
    "    true_hist_ab = histogram(y_true, nbins=25, axis=[1,2]) # hist of (H, W) leaves (B, k, 2, bins)\n",
    "    \n",
    "    pred_hist_ab = histogram(y_pred, nbins=25, axis=[1,2]) # hist of (H, W) leaves (B, k, 2, bins)\n",
    "    \n",
    "    true_hist_ab /= H * W\n",
    "    pred_hist_ab /= H * W\n",
    "    \n",
    "    true_hist_ab = K.clip(true_hist_ab, K.epsilon(), 1-K.epsilon())\n",
    "    pred_hist_ab = K.clip(pred_hist_ab, K.epsilon(), 1-K.epsilon())\n",
    "\n",
    "    # Sum of -plogp (entropy) across bins\n",
    "    true_hist_sum = -tf.math.reduce_sum(true_hist_ab * K.log(true_hist_ab), axis=[3]) # sum of (bins) leaves (B, k, 2)\n",
    "    pred_hist_sum = -tf.math.reduce_sum(pred_hist_ab * K.log(pred_hist_ab), axis=[3]) # sum of (bins) leaves (B, k, 2)\n",
    "    \n",
    "    # Take the difference of the entropies\n",
    "    diff_entropies = tf.math.abs(true_hist_sum - pred_hist_sum)\n",
    "    \n",
    "    # Calc mean abs difference for each k\n",
    "    mad_entropies = tf.math.reduce_mean(diff_entropies, axis=2) # mean of (B, k, 2) leaves (B, k)\n",
    "    \n",
    "    loss_metric = diff\n",
    "    best_k = K.argmin(loss_metric, axis=1)\n",
    "    \n",
    "    # Get indexes for best k\n",
    "    batch_range = K.arange(BATCH_SIZE, dtype='int64')\n",
    "    batch_range = tf.reshape(batch_range, (BATCH_SIZE, 1))\n",
    "    best_k = tf.reshape(best_k, (BATCH_SIZE, 1))\n",
    "    idx = K.concatenate((batch_range, best_k), axis=1)\n",
    "    \n",
    "    # Slice to get mins\n",
    "    min_for_each_n = tf.gather_nd(loss_metric, idx)\n",
    "    diff_entropy_each_n = tf.gather_nd(mad_entropies, idx)\n",
    "\n",
    "    return (tf.math.reduce_sum(min_for_each_n) + (.01 * tf.math.reduce_sum(K.cast(diff_entropy_each_n, dtype=\"float32\")))) #/ 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3abb65ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_k_chroma(y_true, y_pred):\n",
    "    # Shape: (Batch, H, W, k, 2)\n",
    "    y_true = tf.reshape(y_true, (-1, H, W, k, 2))\n",
    "    y_pred = tf.reshape(y_pred, (-1, H, W, k, 2))\n",
    "\n",
    "    truth_chroma = tf.math.reduce_sum(K.square(y_true), axis=(4)) # sum of (2) squared leaves (B, H, W, k)\n",
    "    pred_chroma = tf.math.reduce_sum(K.square(y_pred), axis=(4)) # sum of (2) squared leaves (B, H, W, k)\n",
    "    \n",
    "    sqrt_truth_chroma = K.sqrt(truth_chroma)\n",
    "    sqrt_pred_chroma = K.sqrt(pred_chroma)\n",
    "\n",
    "    diff_chroma = tf.math.abs(truth_chroma - pred_chroma) # still (B, H, W, k)\n",
    "    \n",
    "    sum_diff_chroma = tf.math.reduce_sum(diff_chroma, axis=(1, 2)) # Leave (B, k)\n",
    "    \n",
    "    # Add Sum of Average Differences (SAD) term\n",
    "    diff = y_true - y_pred\n",
    "    diff = tf.math.abs(diff)\n",
    "    diff = tf.math.reduce_mean(diff, axis=(1, 2, 4)) # mean of (H, W, 2) leaves (B, k)\n",
    "    \n",
    "    transf_sum_diff_chroma = sum_diff_chroma / 1e7 # Values between 0-10\n",
    "    transf_diff = diff / 35 / 10 # Values between 0-1\n",
    "    \n",
    "    \n",
    "    loss_metric = transf_sum_diff_chroma + transf_diff # (B, k)\n",
    "    \n",
    "    min_for_each_batch = tf.math.reduce_min(loss_metric, axis=1) # Choose best k for each B\n",
    "    return tf.math.reduce_sum(min_for_each_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "701b2808",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_dist_between_chroma_preds(y_pred):\n",
    "    distances = []\n",
    "    \n",
    "    count = 0\n",
    "    for i in range(k):\n",
    "        for j in range(i + 1, k):\n",
    "            diff = tf.math.reduce_sum(K.square(y_pred[:, :, :, i] - y_pred[:, :, :, j]), axis=(1,2)) # (B)\n",
    "            distances.append(diff)\n",
    "            count += 1\n",
    "    stacked = K.stack(distances, axis=1) \n",
    "    print(\"Stacked:\", stacked.shape)\n",
    "    return -tf.math.reduce_min(stacked, axis=1) # We want to maximize the distance between predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "be11cd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_dist_between_color_preds(y_pred):\n",
    "    # y_pred is (B, H, W, k, 2)\n",
    "    distances = []\n",
    "    \n",
    "    count = 0\n",
    "    for i in range(k):\n",
    "        for j in range(i + 1, k):\n",
    "            diff = y_pred[:, :, :, i, :] - y_pred[:, :, :, j, :] # (B, H, W, 2)\n",
    "            sum_diff = tf.math.reduce_sum(K.square(diff), axis=(1,2,3)) # (B)\n",
    "            distances.append(sum_diff)\n",
    "            count += 1\n",
    "    stacked = K.stack(distances, axis=1)\n",
    "    print(\"Stacked:\", stacked.shape)\n",
    "    return -tf.math.reduce_min(stacked, axis=1) # We want to maximize the distance between predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5fbff394",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_dist_between_hue_angles(y_pred):\n",
    "    # y_pred is (B, H, W, k, 2)\n",
    "    distances = []\n",
    "    \n",
    "    for i in range(k):\n",
    "        for j in range(i + 1, k):\n",
    "            # https://stackoverflow.com/a/2007279\n",
    "            angle_i = tf.math.atan2(y_pred[:, :, :, i, 0], y_pred[:, :, :, i, 1])\n",
    "            angle_j = tf.math.atan2(y_pred[:, :, :, j, 0], y_pred[:, :, :, j, 1])\n",
    "            diff = tf.math.atan2(tf.math.sin(angle_i - angle_j), tf.math.cos(angle_i - angle_j)) # (B, H, W)\n",
    "            sum_diff = tf.math.reduce_sum(K.square(diff), axis=(1,2)) # (B)\n",
    "            distances.append(sum_diff)\n",
    "    stacked = K.stack(distances, axis=1)\n",
    "    print(\"Stacked:\", stacked.shape)\n",
    "    return -tf.math.reduce_min(stacked, axis=1) # We want to maximize the distance between predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fcaf7632",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_k_chroma_with_pairwise_difference_k(y_true, y_pred):\n",
    "    # Shape: (Batch, H, W, k, 2)\n",
    "    y_true = tf.reshape(y_true, (-1, H, W, k, 2))\n",
    "    y_pred = tf.reshape(y_pred, (-1, H, W, k, 2))\n",
    "\n",
    "    truth_chroma = tf.math.reduce_sum(K.square(y_true), axis=(4)) # sum of (2) squared leaves (B, H, W, k)\n",
    "    pred_chroma = tf.math.reduce_sum(K.square(y_pred), axis=(4)) # sum of (2) squared leaves (B, H, W, k)\n",
    "    \n",
    "    sqrt_truth_chroma = K.sqrt(truth_chroma)\n",
    "    sqrt_pred_chroma = K.sqrt(pred_chroma)\n",
    "    \n",
    "    diff_chroma = tf.math.abs(sqrt_truth_chroma - sqrt_pred_chroma) # still (B, H, W, k)\n",
    "    \n",
    "    sum_diff_chroma = tf.math.reduce_sum(diff_chroma, axis=(1, 2)) # Leave (B, k)\n",
    "\n",
    "    transf_sum_diff_chroma = sum_diff_chroma / 1e7 # Values between 0-10\n",
    "    \n",
    "    min_dist_hue = min_dist_between_hue_angles(y_pred) / 1e7 / 2 / 2 # Values between 0 and -10\n",
    "\n",
    "    return tf.math.reduce_sum(transf_sum_diff_chroma) + tf.math.reduce_sum(min_dist_hue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2cb9f784",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_k_diff_minus_k_std(y_true, y_pred):\n",
    "    # Shape: (Batch, H, W, k, 2)\n",
    "    y_true = tf.reshape(y_true, (-1, H, W, k, 2))\n",
    "    y_pred = tf.reshape(y_pred, (-1, H, W, k, 2))\n",
    "\n",
    "    print(\"true:\", y_true.shape)\n",
    "    print(\"pred:\", y_pred.shape)\n",
    "\n",
    "    diff = y_true - y_pred\n",
    "    diff = tf.math.abs(diff)\n",
    "    diff = tf.math.reduce_mean(diff, axis=(1, 2, 4))\n",
    "    std_dev = K.std(y_pred, axis=(1, 2, 4))\n",
    "    \n",
    "    loss_metric = diff - (0.05 * std_dev)\n",
    "    min_for_each_batch = tf.math.reduce_min(loss_metric, axis=1)\n",
    "    return tf.math.reduce_sum(min_for_each_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "49563b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_noise(n_samples, noise_dim):\n",
    "    X = np.random.normal(0, 1, size=(n_samples, noise_dim))\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f8ca4e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_samples(batchidx, save_path):\n",
    "    fig, axs = plt.subplots(5, 6, figsize=(10,6))\n",
    "    plt.subplots_adjust(hspace=0.3, wspace=0.1)\n",
    "    #fig, axs = plt.subplots(5, 6)\n",
    "    #fig.tight_layout()\n",
    "    for classlabel in range(10):\n",
    "        row = int(classlabel / 2)\n",
    "        coloffset = (classlabel % 2) * 3\n",
    "        lbls = one_hot_encode([classlabel] * 3)\n",
    "        noise = generate_noise(3, 100)\n",
    "        gen_imgs = generator.predict([noise, lbls])\n",
    "\n",
    "        for i in range(3):\n",
    "            # Dont scale the images back, let keras handle it\n",
    "            img = image.array_to_img(gen_imgs[i], scale=True)\n",
    "            axs[row,i+coloffset].imshow(img)\n",
    "            axs[row,i+coloffset].axis('off')\n",
    "            if i ==1:\n",
    "                axs[row,i+coloffset].set_title(tags[classlabel])\n",
    "    plt.savefig(save_path + '.jpg')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ebbb58c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(g_losses, d_losses, epoch_num, scale_factor):\n",
    "    # Scale axis so it lines up with epoch_num\n",
    "    x_axis = [epoch_num * x / (len(g_losses) - 1) for x in range(len(g_losses))]\n",
    "    \n",
    "    g_scaled = [x * scale_factor for x in g_losses]\n",
    "    \n",
    "    ax = plt.subplot(1,1,1)\n",
    "    ax.plot(x_axis, g_scaled, label=\"Generator\")\n",
    "    ax.plot(x_axis, d_losses, label=\"Discriminator\")\n",
    "    plt.title(\"Losses After \" + str(epoch_num) + \" Epochs\")\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    ax.legend(handles, labels)\n",
    "    plt.savefig(save_path + \"Losses.jpg\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c6e6aabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test name - used in file path, log, email notification\n",
    "# test_name = \"Dataset_final_k_1_with_noise\"\n",
    "test_name = \"Dataset\"\n",
    "\n",
    "# Training parameters\n",
    "n_images = -1    # Number of images to load for training; recommended = 512\n",
    "BATCH_SIZE = 6\n",
    "N_EPOCHS = 100\n",
    "retrain = False\n",
    "use_noise = True\n",
    "\n",
    "# Weights of G / D Loss in Full Model\n",
    "'''\n",
    "The GAN learns through both a generator and discriminator loss\n",
    "Tweak weights to minimize or maximize importance of either\n",
    "Remove either loss by changing coefficient to 0\n",
    "Default G:D 100:1\n",
    "''' \n",
    "gen_loss_coeff = 100.0\n",
    "disc_loss_coeff = 1.0\n",
    "\n",
    "# Generator loss function\n",
    "gen_loss = min_k_diff\n",
    "# gen_loss = min_k_diff_plus_k_entropy_diff\n",
    "# gen_loss = min_k_chroma\n",
    "# gen_loss = min_k_diff_minus_k_std\n",
    "# gen_loss = min_k_chroma_with_pairwise_difference_k\n",
    "\n",
    "\n",
    "# Dataset\n",
    "# dataset = 'circle_pairs_equal_l_red_blue/'\n",
    "# dataset = 'circle_pairs/'\n",
    "# dataset = 'new_circles/'\n",
    "# dataset = '../Colorization_GAN/circle_pairs/'\n",
    "# dataset = 'lsun/'\n",
    "dataset = 'Dataset'\n",
    "\n",
    "# Number of predictions (modes)\n",
    "k = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ee5e45",
   "metadata": {},
   "source": [
    "### 5.Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3e8e2bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folder to store output\n",
    "generic_output_folder = \"Output\"\n",
    "new_output_folder = test_name + \"/\"\n",
    "save_path = generic_output_folder + new_output_folder\n",
    "\n",
    "# Ensure output can save in desired location\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "85e5c17f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ImageDataGenerator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m augment \u001b[38;5;241m=\u001b[39m \u001b[43mImageDataGenerator\u001b[49m(\n\u001b[0;32m      2\u001b[0m     rotation_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[0;32m      3\u001b[0m     width_shift_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m,\n\u001b[0;32m      4\u001b[0m     height_shift_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m,\n\u001b[0;32m      5\u001b[0m     horizontal_flip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ImageDataGenerator' is not defined"
     ]
    }
   ],
   "source": [
    "augment = ImageDataGenerator(\n",
    "    rotation_range=5,\n",
    "    width_shift_range=0.05,\n",
    "    height_shift_range=0.05,\n",
    "    horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c140f46f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_discriminator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# 1. Discriminator\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Calculate output shape of D (PatchGAN)\u001b[39;00m\n\u001b[0;32m     10\u001b[0m patch \u001b[38;5;241m=\u001b[39m H \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m4\u001b[39m \u001b[38;5;66;03m# Input size gets cut in half 4 times\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m discriminator \u001b[38;5;241m=\u001b[39m \u001b[43mget_discriminator\u001b[49m(H, W, k)\n\u001b[0;32m     12\u001b[0m discriminator\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiscrim_model\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;66;03m# Need a name for the loss dictionary below\u001b[39;00m\n\u001b[0;32m     13\u001b[0m discriminator\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mAdam(\u001b[38;5;241m2e-4\u001b[39m, \u001b[38;5;241m0.5\u001b[39m), loss\u001b[38;5;241m=\u001b[39mdiscrim_loss, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'get_discriminator' is not defined"
     ]
    }
   ],
   "source": [
    "# GAN creation\n",
    "H = W = 256\n",
    "\n",
    "# Discriminator loss - MSE seems to produce better results\n",
    "#discrim_loss = 'binary_crossentropy'\n",
    "discrim_loss = 'mse'\n",
    "\n",
    "# 1. Discriminator\n",
    "# Calculate output shape of D (PatchGAN)\n",
    "patch = H // 2**4 # Input size gets cut in half 4 times\n",
    "discriminator = get_discriminator(H, W, k)\n",
    "discriminator.name = 'discrim_model' # Need a name for the loss dictionary below\n",
    "discriminator.compile(optimizer=Adam(2e-4, 0.5), loss=discrim_loss, metrics=['accuracy'])\n",
    "discriminator.trainable = False # For the combined model we will only train the generator\n",
    "print(\"\\n\")\n",
    "\n",
    "# 2. Generator\n",
    "generator = get_generator(H, W, k)\n",
    "generator.name = 'gen_model' # Need a name for the loss dictionary below\n",
    "\n",
    "# 3. GAN\n",
    "gan_noise_in = Input(shape=(100,))\n",
    "gan_condition_in = Input(shape=(H, W, 1))\n",
    "\n",
    "# By conditioning on L generate a fake version of AB\n",
    "fake_AB = generator([gan_noise_in, gan_condition_in])\n",
    "\n",
    "# Discriminator determines validity of AB images / L pairs\n",
    "print(\"fake_ab:\", fake_AB.shape)\n",
    "\n",
    "print(\"gan_condition_in:\", gan_condition_in.shape)\n",
    "\n",
    "valid = discriminator([fake_AB, gan_condition_in])\n",
    "\n",
    "losses = {'gen_model': gen_loss, # used to be 'mae'\n",
    "          'discrim_model': discrim_loss}\n",
    "loss_weights = {'gen_model': gen_loss_coeff, 'discrim_model': disc_loss_coeff}\n",
    "\n",
    "gan = Model(inputs=[gan_noise_in, gan_condition_in], outputs=[fake_AB, valid])\n",
    "gan.compile(optimizer=Adam(2e-4, 0.5), loss=losses, loss_weights=loss_weights)\n",
    "gan.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "44cc998c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the weights from a previous model\n",
    "\n",
    "if retrain:\n",
    "    discriminator.load_weights(\"TA-2023-2024-4/Output/lsun_Discriminator_Weights_Epoch_100.h5\")\n",
    "\n",
    "    gan.load_weights(\"TA-2023-2024-4/Output/lsun_GAN_Weights_Epoch_100.h5\")\n",
    "    \n",
    "    # Warn the user with printout\n",
    "    print(\"* * *\")\n",
    "    print(\"LOADING PRE-EXISTING MODEL!!!\")\n",
    "    print(\"* * *\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cf4253fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset directory exists: True\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tqdm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 11\u001b[0m\n\u001b[0;32m      7\u001b[0m fake_labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((BATCH_SIZE, patch, patch, k))\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset directory exists:\u001b[39m\u001b[38;5;124m\"\u001b[39m, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtqdm\u001b[49m(\u001b[38;5;28mrange\u001b[39m(N_EPOCHS), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining GAN...\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     13\u001b[0m     cum_d_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.\u001b[39m\n\u001b[0;32m     14\u001b[0m     cum_g_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tqdm' is not defined"
     ]
    }
   ],
   "source": [
    "g_losses = []\n",
    "d_losses = []\n",
    "d_accuracies = []\n",
    "\n",
    "# PatchGAN loss ground truths\n",
    "true_labels = np.ones((BATCH_SIZE, patch, patch, k))\n",
    "fake_labels = np.zeros((BATCH_SIZE, patch, patch, k))\n",
    "\n",
    "print(\"Dataset directory exists:\", os.path.exists(\"Dataset\"))\n",
    "\n",
    "for epoch in tqdm(range(N_EPOCHS), desc=\"Training GAN...\"):\n",
    "\n",
    "    cum_d_loss = 0.\n",
    "    cum_g_loss = 0.\n",
    "    cum_d_acc = 0.\n",
    "\n",
    "    pbar_message = \"Epoch \" + str(epoch + 1)\n",
    "    flow_aug_imgs = augment.flow_from_directory(directory= \"Dataset\", batch_size=BATCH_SIZE, class_mode=None)\n",
    "    num_batches = len(flow_aug_imgs)\n",
    "    # print(flow_aug_imgs)\n",
    "    print(type(flow_aug_imgs), \"Number of batches:\", len(flow_aug_imgs))\n",
    "\n",
    "    \n",
    "\n",
    "    for batch_idx in tqdm(range(num_batches-1), desc=pbar_message):\n",
    "        # Random indices for training examples\n",
    "#         idx = randint(0, y_train.shape[0], BATCH_SIZE)\n",
    "\n",
    "        l_channel, ab_channel = preprocess_batch_from_aug(flow_aug_imgs[batch_idx])\n",
    "        \n",
    "        # X is L channel, Y is AB\n",
    "        # Get the next set of images to be used in this iteration and augment them\n",
    "        # Get random seed\n",
    "#         seed = np.random.randint(0,1000000000)\n",
    "        \n",
    "\n",
    "        # Augment data; ensure they're augmented the same way\n",
    "#         augment_X = augment.flow(X_train[idx], seed=seed)[0]\n",
    "#         augment_Y = augment.flow(y_train[idx], seed=seed)[0]\n",
    "\n",
    "        # Convert type to what it was before augmentation\n",
    "#         l_channel, ab_channel = augment_X.astype(\"float64\"), augment_Y.astype(\"float64\")\n",
    "        \n",
    "#         l_channel = X_train[idx]\n",
    "#         ab_channel = y_train[idx]\n",
    "\n",
    "\n",
    "        \n",
    "        ab_vis = ab_channel.copy()\n",
    "        \n",
    "        ab_channel = np.expand_dims(ab_channel, axis=3)\n",
    "        ab_channel = np.repeat(ab_channel, k, axis=3) # repeat along the newly created k axis\n",
    "        ab_channel = np.reshape(ab_channel, (-1, H, W, 2*k)) # stack the channels so things are the right shape        \n",
    "\n",
    "        noise_data = generate_noise(BATCH_SIZE, 100)\n",
    "\n",
    "        # Generate fake AB channels from L\n",
    "        fake_ab = generator.predict([noise_data, l_channel])\n",
    "\n",
    "        # Train discriminator on fake and real data\n",
    "        d_loss_fake, d_acc_fake = discriminator.train_on_batch([fake_ab, l_channel], fake_labels)\n",
    "        d_loss_true, d_acc_true = discriminator.train_on_batch([ab_channel, l_channel], true_labels)\n",
    "        d_loss = (d_loss_fake + d_loss_true) / 2\n",
    "        d_acc = (d_acc_fake + d_acc_true) / 2\n",
    "        \n",
    "        cum_d_loss += d_loss\n",
    "        cum_d_acc += d_acc\n",
    "\n",
    "        # Train generator (by way of the whole gan)\n",
    "        # Note that there are 3 losses. The first should be the\n",
    "        # weighted sum of the adversarial and L1 loss values for the batch.\n",
    "        # Ref: https://machinelearningmastery.com/how-to-implement-pix2pix-gan-models-from-scratch-with-keras/\n",
    "        try:\n",
    "            g_loss, _, _ = gan.train_on_batch([noise_data, l_channel], [ab_channel, true_labels])\n",
    "            cum_g_loss += g_loss\n",
    "    \n",
    "            g_losses.append(g_loss)\n",
    "            d_losses.append(d_loss)\n",
    "            d_accuracies.append(d_acc)\n",
    "\n",
    "            print('Epoch: {:04d}, Gen Loss: {:0.4f}, Discrim Loss: {:0.4f}, Discrim Acc: {:0.4f}'.format(\n",
    "                epoch+1, cum_g_loss/num_batches, cum_d_loss/num_batches, cum_d_acc/num_batches))\n",
    "            sample_title = \"Epoch \" + str(epoch+1)\n",
    "        \n",
    "            sample_imgs_new(ab_vis, l_channel, fake_ab, 1, True, sample_title, False)\n",
    "            \n",
    "            plot_losses(g_losses, d_losses, epoch + 1, .01)\n",
    "        \n",
    "            if (epoch + 1) % 5 == 0: \n",
    "                gan.save_weights('GAN_Weights_Epoch_' + str(epoch + 1) + '.weights.h5')\n",
    "                discriminator.save_weights('Discriminator_Weights_Epoch_' + str(epoch + 1) + '.weights.h5')\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "85adb1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_generator(generator, truth, gray, n_test_batch, n_predictions, shouldSave, sample_title=None):\n",
    "    # Each test image will use the same random noise to show various predictions\n",
    "    noise_arrays = generate_noise(n_test_batch, 100)\n",
    "    \n",
    "    # Store all merged\n",
    "    merged_predictions = []\n",
    "    merged_grays = []\n",
    "    merged_truths = []\n",
    "    \n",
    "    predictions = generator.predict([noise_arrays, gray[:n_test_batch]])\n",
    "    \n",
    "    # For each image...\n",
    "    for i in range(n_test_batch):\n",
    "        # Get prediction, merge\n",
    "        merged = []\n",
    "        # For each k...\n",
    "        for j in range(k):\n",
    "            # Generate a prediction...\n",
    "            prediction = predictions[i,:,:,2*j:(2*j)+1].astype(float64)\n",
    "            # And merge it to be a 3 channel image\n",
    "            merged.append(cv2.merge((gray[i], prediction)))\n",
    "        merged_predictions.append(merged) \n",
    "        \n",
    "        # Merge the grayscale channels\n",
    "        merged_grays.append(cv2.merge((gray[i], gray[i], gray[i])))\n",
    "        \n",
    "        # Merge the l and ab to create truth\n",
    "        merged_truths.append(cv2.merge((gray[i], truth[i])))\n",
    "\n",
    "    \n",
    "    r = truth.shape[1]\n",
    "    c = truth.shape[2]\n",
    "    \n",
    "    # Loop through sets of test images\n",
    "    for i in range(len(truth)//n_test_batch):\n",
    "        start = 3*i\n",
    "        end = start + 3\n",
    "        \n",
    "        # Create figure\n",
    "        columns = 2 + n_predictions\n",
    "        figure = np.zeros([r * n_test_batch + (15 * (n_test_batch-1)), (c * columns) + 5 * (columns-1), 3], dtype=np.uint8)\n",
    "        figure += 255\n",
    "        start_r = 0\n",
    "\n",
    "        # Place images in figure\n",
    "        for j in range(n_test_batch):  \n",
    "            figure[start_r:start_r + r, :c] = cv2.cvtColor(deprocess_image(merged_truths[start + j]), cv2.COLOR_LAB2RGB)\n",
    "            figure[start_r:start_r + r, c + 5:2*c + 5] = deprocess_image(merged_grays[start + j])\n",
    "            # Loop through and place multiple predictions\n",
    "            for pred_index in range(n_predictions):\n",
    "                current_predictions = merged_predictions[start + j]\n",
    "                figure[start_r:start_r + r, (2+pred_index)*(c+5):(3+pred_index)*c + (5*(2+pred_index))] = \\\n",
    "                    cv2.cvtColor(deprocess_image(current_predictions[pred_index]), cv2.COLOR_LAB2RGB)\n",
    "            start_r += r + 15\n",
    "        img = Image.fromarray(figure, \"RGB\")\n",
    "\n",
    "        dpi = plt.rcParams['figure.dpi']\n",
    "        height, width, _ = figure.shape\n",
    "\n",
    "        preview = plt.figure(figsize=(6,4))\n",
    "        plt.imshow(img)\n",
    "        plt.axis(\"off\")\n",
    "        if sample_title != None:\n",
    "            title = sample_title + \" \" + str(i+1)\n",
    "            plt.title(title, fontsize = 'large')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "        if shouldSave:\n",
    "            figsize = width / float(dpi), height / float(dpi)\n",
    "\n",
    "            full = plt.figure(figsize = figsize)\n",
    "            plt.imshow(img)\n",
    "            plt.axis(\"off\")\n",
    "            plt.title(title, fontsize = 25)\n",
    "            if sample_title != None:\n",
    "                plt.savefig(save_path + sample_title + \" \" + str(i+1) + \".png\")\n",
    "            else:\n",
    "                plt.savefig(save_path + \"Test Images \" + str(i+1) + \".png\")\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5ee4092b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_images' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# num_test_imgs = 20\u001b[39;00m\n\u001b[0;32m      8\u001b[0m desired_num_noise_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m---> 10\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mload_images\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m'\u001b[39m, num_test_imgs)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Separate into inputs (X) and outputs (Y)\u001b[39;00m\n\u001b[0;32m     13\u001b[0m all_y_test, all_X_test \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mab\u001b[39m\u001b[38;5;124m'\u001b[39m], data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'load_images' is not defined"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# THIS CELL DISPLAYS TEST IMAGES WITH NOISE\n",
    "# =========================================\n",
    "\n",
    "# Read in some test images\n",
    "num_test_imgs = 63\n",
    "# num_test_imgs = 20\n",
    "desired_num_noise_predictions = 3\n",
    "\n",
    "data = load_images('Dataset', num_test_imgs)\n",
    "\n",
    "# Separate into inputs (X) and outputs (Y)\n",
    "all_y_test, all_X_test = data['ab'], data['l']\n",
    "\n",
    "all_y_test = np.repeat(all_y_test, desired_num_noise_predictions, axis=0)\n",
    "all_X_test = np.repeat(all_X_test, desired_num_noise_predictions, axis=0)\n",
    "\n",
    "num_test_imgs *= desired_num_noise_predictions\n",
    "\n",
    "noise = generate_noise(num_test_imgs, 100)\n",
    "predictions = generator.predict([noise, all_X_test])\n",
    "\n",
    "# Only used to divide up batch, truly only one img shown\n",
    "num_to_display = 3\n",
    "\n",
    "old_k = k\n",
    "k *= desired_num_noise_predictions\n",
    "for i in range(num_test_imgs // num_to_display):\n",
    "    start = num_to_display * i\n",
    "    end = start + num_to_display\n",
    "    \n",
    "    truth_ab = all_y_test[start:end]\n",
    "    grays = all_X_test[start:end]\n",
    "    predicted_ab = predictions[start:end]\n",
    "    predicted_ab = np.concatenate((predicted_ab[:1], predicted_ab[1:2], predicted_ab[2:]), axis=-1)\n",
    "    \n",
    "    title = \"Test Images \" + str(i+1)\n",
    "    sample_imgs_new(truth_ab, grays, predicted_ab, 1, True, title)\n",
    "\n",
    "k = old_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2745563f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_images' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m num_test_imgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m150\u001b[39m\n\u001b[0;32m      7\u001b[0m num_test_imgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m\n\u001b[1;32m----> 9\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mload_images\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m'\u001b[39m, num_test_imgs)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Separate into inputs (X) and outputs (Y)\u001b[39;00m\n\u001b[0;32m     12\u001b[0m all_y_test, all_X_test \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mab\u001b[39m\u001b[38;5;124m'\u001b[39m], data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'load_images' is not defined"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# THIS CELL DISPLAYS TEST IMAGES WITHOUT NOISE\n",
    "# ============================================\n",
    "\n",
    "# Read in some test images\n",
    "num_test_imgs = 150\n",
    "num_test_imgs = 20\n",
    "\n",
    "data = load_images('Dataset', num_test_imgs)\n",
    "\n",
    "# Separate into inputs (X) and outputs (Y)\n",
    "all_y_test, all_X_test = data['ab'], data['l']\n",
    "\n",
    "noise = generate_noise(num_test_imgs, 100)\n",
    "predictions = generator.predict([noise, all_X_test])\n",
    "\n",
    "num_to_display = 3\n",
    "for i in range(num_test_imgs // num_to_display):\n",
    "    start = num_to_display * i\n",
    "    end = start + num_to_display\n",
    "    \n",
    "    truth_ab = all_y_test[start:end]\n",
    "    grays = all_X_test[start:end]\n",
    "    predicted_ab = predictions[start:end]\n",
    "    \n",
    "    title = \"Test Images \" + str(i+1)\n",
    "    sample_imgs_new(truth_ab, grays, predicted_ab, num_to_display, True, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0101bd56",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_images' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Read in some test images\u001b[39;00m\n\u001b[0;32m      8\u001b[0m num_test_imgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m15\u001b[39m\n\u001b[1;32m----> 9\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mload_images\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Separate into inputs (X) and outputs (Y)\u001b[39;00m\n\u001b[0;32m     12\u001b[0m all_y_test, all_X_test \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mab\u001b[39m\u001b[38;5;124m'\u001b[39m], data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'load_images' is not defined"
     ]
    }
   ],
   "source": [
    "# =================================================================\n",
    "# THIS CELL DISPLAYS TEST IMAGES WITHOUT NOISE, WITH PLACES2 LABELS\n",
    "# =================================================================\n",
    "\n",
    "categories = ['badlands', 'butte', 'canyon', 'cliff', 'field-wild', 'forest-broadleaf', 'forest_path', 'lake-natural', 'mountain', 'tundra']\n",
    "\n",
    "# Read in some test images\n",
    "num_test_imgs = 15\n",
    "data = load_images('Dataset')\n",
    "\n",
    "# Separate into inputs (X) and outputs (Y)\n",
    "all_y_test, all_X_test = data['ab'], data['l']\n",
    "\n",
    "imgs_per_cat = len(all_y_test) // len(categories) #kemungkinan categorinya dpt diubah dari sini\n",
    "\n",
    "for i, cat in enumerate(categories):\n",
    "    y_test = all_y_test[i*imgs_per_cat:i*imgs_per_cat + num_test_imgs]\n",
    "    X_test = all_X_test[i*imgs_per_cat:i*imgs_per_cat + num_test_imgs]\n",
    "    \n",
    "    noise = generate_noise(num_test_imgs, 100)\n",
    "    predictions = generator.predict([noise, X_test])\n",
    "\n",
    "    num_to_display = 3\n",
    "    for i in range(num_test_imgs // num_to_display):\n",
    "        start = num_to_display * i\n",
    "        end = start + num_to_display\n",
    "\n",
    "        truth_ab = y_test[start:end]\n",
    "        grays = X_test[start:end]\n",
    "        predicted_ab = predictions[start:end]\n",
    "\n",
    "        title = \"Test Images \" + cat.capitalize() + \" \" + str(i+1)\n",
    "        sample_imgs_new(truth_ab, grays, predicted_ab, num_to_display, True, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "08f5acf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_loss(y_true, y_pred):\n",
    "    y_true = np.reshape(y_true, (-1, H, W, k, 2))\n",
    "    y_pred = np.reshape(y_pred, (-1, H, W, k, 2))\n",
    "\n",
    "    diff = y_true - y_pred\n",
    "    diff = np.abs(diff)\n",
    "    diff = np.mean(diff, axis=(1, 2, 4))\n",
    "    min_for_each_n = np.min(diff, axis=1)\n",
    "    min_k = np.argmin(diff, axis=1).item()\n",
    "\n",
    "    return np.sum(min_for_each_n), min_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "637926bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Changed to use test data\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m noise \u001b[38;5;241m=\u001b[39m generate_noise(\u001b[38;5;28mlen\u001b[39m(\u001b[43mall_X_test\u001b[49m), \u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m      4\u001b[0m predictions \u001b[38;5;241m=\u001b[39m generator\u001b[38;5;241m.\u001b[39mpredict([noise, all_X_test])\n\u001b[0;32m      6\u001b[0m losses \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mNameError\u001b[0m: name 'all_X_test' is not defined"
     ]
    }
   ],
   "source": [
    "# Changed to use test data\n",
    "\n",
    "noise = generate_noise(len(all_X_test), 100)\n",
    "predictions = generator.predict([noise, all_X_test])\n",
    "\n",
    "losses = []\n",
    "k_chosen = {}\n",
    "for curr_k in range(k):\n",
    "    k_chosen[curr_k] = 0\n",
    "\n",
    "for i, prediction in enumerate(predictions):\n",
    "    truth_ab = all_y_test[i]\n",
    "    truth_ab = np.expand_dims(truth_ab, axis=0)\n",
    "    truth_ab = np.expand_dims(truth_ab, axis=3)\n",
    "    truth_ab = np.repeat(truth_ab, k, axis=3) # repeat along the newly created k axis\n",
    "    truth_ab = np.reshape(truth_ab, (-1, H, W, 2*k)) # stack the channels so things are the right shape\n",
    "    \n",
    "    loss, curr_k = np_loss(truth_ab, prediction)\n",
    "    losses.append(loss)\n",
    "    \n",
    "    if curr_k not in k_chosen:\n",
    "        k_chosen[curr_k] = 1\n",
    "    else:\n",
    "        k_chosen[curr_k] += 1\n",
    "    \n",
    "avg_loss = sum(losses) / len(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286a09ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa25030",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Writing log file...\")\n",
    "filename = save_path + \"log.txt\"\n",
    "log_file = open(filename, \"w+\")\n",
    "log_file.write(\"Test Name: \" + test_name + \"\\n\\n\")\n",
    "\n",
    "log_file.write(\"Number of Images: \" + str(n_images) + \"\\n\")\n",
    "log_file.write(\"Number of Epochs: \" + str(N_EPOCHS) + \"\\n\\n\")\n",
    "\n",
    "log_file.write(\"Elapsed Time: \" + sec_to_time(time_end-time_start) + \"\\n\\n\")\n",
    "\n",
    "log_file.write(\"Start Date: \" + datetime.fromtimestamp(time_start).strftime(\"%b %d, %Y %I:%M:%S %p\") + \"\\n\")\n",
    "log_file.write(\"End Date: \" + datetime.fromtimestamp(time_end).strftime(\"%b %d, %Y %I:%M:%S %p\\n\\n\"))\n",
    "\n",
    "log_file.write(\"K: \" + str(k) + \"\\n\")\n",
    "log_file.write(\"Avg loss: \" + str(avg_loss) + \"\\n\")\n",
    "log_file.write(\"Times K has been chosen: \")\n",
    "\n",
    "percents = [value / sum(list(map(int, k_chosen.values()))) for value in k_chosen.values()]\n",
    "for percent in percents:\n",
    "    log_file.write(str(percent) + \" \")\n",
    "log_file.close()\n",
    "print(\"Saved to \" + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cce584",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(np.arange(k), percents)\n",
    "plt.title(\"Frequency of Each K Being the Minimized Loss\")\n",
    "plt.xlabel(\"K\")\n",
    "plt.ylabel(\"% Chosen\")\n",
    "plt.show()\n",
    "df = pd.DataFrame({\n",
    "    'K': np.arange(k),\n",
    "    'Percentage Chosen': percents\n",
    "})\n",
    "\n",
    "print(df)\n",
    "# # print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb356402",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
